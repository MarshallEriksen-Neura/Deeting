#!/usr/bin/env python3
"""
å›½é™…åŒ–å®ç°æ£€æŸ¥è„šæœ¬
æ£€æŸ¥æ‰€æœ‰ç»„ä»¶æ˜¯å¦æ­£ç¡®ä½¿ç”¨ useI18n Hook å’Œæ˜¯å¦æœ‰ç¡¬ç¼–ç å­—ç¬¦ä¸²
"""

import os
import re
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple

# å®šä¹‰è¦æ£€æŸ¥çš„ç›®å½•
DIRS_TO_CHECK = [
    "components/chat",
    "components/image",
    "components/common",
]

# å›½é™…åŒ–æ–‡ä»¶ç›®å½•
I18N_DIR = "messages"

def find_tsx_files(base_dir: str) -> List[str]:
    """æŸ¥æ‰¾æ‰€æœ‰ .tsx æ–‡ä»¶"""
    tsx_files = []
    for root, dirs, files in os.walk(base_dir):
        # è·³è¿‡ node_modules
        if 'node_modules' in root:
            continue
        for file in files:
            if file.endswith('.tsx'):
                tsx_files.append(os.path.join(root, file))
    return sorted(tsx_files)

def check_use_i18n(file_path: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä½¿ç”¨äº† useI18n æˆ– useTranslations"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            return 'useI18n' in content or 'useTranslations' in content
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return False

def find_chinese_strings(file_path: str) -> List[Tuple[int, str]]:
    """æŸ¥æ‰¾æ–‡ä»¶ä¸­çš„ä¸­æ–‡å­—ç¬¦ä¸²ï¼ˆæ’é™¤æ³¨é‡Šï¼‰"""
    chinese_strings = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            in_multiline_comment = False
            
            for i, line in enumerate(lines, 1):
                # è·³è¿‡å¤šè¡Œæ³¨é‡Š
                if '/*' in line:
                    in_multiline_comment = True
                if '*/' in line:
                    in_multiline_comment = False
                    continue
                if in_multiline_comment:
                    continue
                
                # è·³è¿‡å•è¡Œæ³¨é‡Š
                if line.strip().startswith('//'):
                    continue
                
                # æŸ¥æ‰¾ä¸­æ–‡å­—ç¬¦
                if re.search(r'[\u4e00-\u9fa5]', line):
                    # æ’é™¤æ³¨é‡Šä¸­çš„ä¸­æ–‡
                    code_part = line.split('//')[0]
                    if re.search(r'[\u4e00-\u9fa5]', code_part):
                        chinese_strings.append((i, line.strip()))
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
    
    return chinese_strings

def find_hardcoded_english(file_path: str) -> List[Tuple[int, str]]:
    """æŸ¥æ‰¾ JSX ä¸­å¯èƒ½çš„ç¡¬ç¼–ç è‹±æ–‡å­—ç¬¦ä¸²"""
    hardcoded_strings = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
            for i, line in enumerate(lines, 1):
                # è·³è¿‡æ³¨é‡Š
                if line.strip().startswith('//') or line.strip().startswith('*'):
                    continue
                
                # æŸ¥æ‰¾ JSX ä¸­çš„æ–‡æœ¬å†…å®¹ >text<
                # æ’é™¤å¸¸è§çš„æŠ€æœ¯æœ¯è¯­å’Œç»„ä»¶å
                matches = re.findall(r'>([A-Z][a-zA-Z\s]+)<', line)
                for match in matches:
                    # æ’é™¤å•ä¸ªå¤§å†™å­—æ¯ã€ç»„ä»¶åç­‰
                    if len(match) > 2 and not match.isupper():
                        # æ’é™¤å¸¸è§çš„æŠ€æœ¯æœ¯è¯­
                        if match not in ['Button', 'Input', 'Card', 'Dialog', 'Select', 'Table']:
                            hardcoded_strings.append((i, line.strip()))
                            break
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
    
    return hardcoded_strings

def load_i18n_keys(locale: str) -> Set[str]:
    """åŠ è½½æŒ‡å®šè¯­è¨€çš„æ‰€æœ‰ i18n keys"""
    keys = set()
    i18n_path = Path(I18N_DIR) / locale
    
    if not i18n_path.exists():
        return keys
    
    for json_file in i18n_path.glob('*.json'):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # é€’å½’æå–æ‰€æœ‰ keys
                def extract_keys(obj, prefix=''):
                    if isinstance(obj, dict):
                        for key, value in obj.items():
                            new_prefix = f"{prefix}.{key}" if prefix else key
                            if isinstance(value, dict):
                                extract_keys(value, new_prefix)
                            else:
                                keys.add(new_prefix)
                
                extract_keys(data)
        except Exception as e:
            print(f"Error loading {json_file}: {e}")
    
    return keys

def compare_i18n_completeness(zh_keys: Set[str], en_keys: Set[str]) -> Dict[str, List[str]]:
    """æ¯”è¾ƒä¸­è‹±æ–‡ i18n keys çš„å®Œæ•´æ€§"""
    return {
        'missing_in_en': sorted(list(zh_keys - en_keys)),
        'missing_in_zh': sorted(list(en_keys - zh_keys)),
        'common': sorted(list(zh_keys & en_keys))
    }

def generate_report():
    """ç”Ÿæˆå®Œæ•´çš„æ£€æŸ¥æŠ¥å‘Š"""
    print("=" * 80)
    print("å›½é™…åŒ–å®ç°æ£€æŸ¥æŠ¥å‘Š")
    print("=" * 80)
    print()
    
    total_files = 0
    total_with_i18n = 0
    total_without_i18n = 0
    files_with_chinese = []
    files_with_hardcoded_english = []
    
    # 1. æ£€æŸ¥æ¯ä¸ªç›®å½•
    for dir_name in DIRS_TO_CHECK:
        print(f"\n{'=' * 80}")
        print(f"æ£€æŸ¥ç›®å½•: {dir_name}")
        print(f"{'=' * 80}\n")
        
        tsx_files = find_tsx_files(dir_name)
        files_with_i18n = []
        files_without_i18n = []
        
        for file_path in tsx_files:
            total_files += 1
            if check_use_i18n(file_path):
                files_with_i18n.append(file_path)
                total_with_i18n += 1
            else:
                files_without_i18n.append(file_path)
                total_without_i18n += 1
        
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - æ€»ç»„ä»¶æ–‡ä»¶æ•°: {len(tsx_files)}")
        print(f"  - ä½¿ç”¨ useI18n çš„æ–‡ä»¶æ•°: {len(files_with_i18n)}")
        print(f"  - æœªä½¿ç”¨ useI18n çš„æ–‡ä»¶æ•°: {len(files_without_i18n)}")
        if len(tsx_files) > 0:
            print(f"  - ä½¿ç”¨ç‡: {len(files_with_i18n) / len(tsx_files) * 100:.1f}%")
        else:
            print(f"  - ä½¿ç”¨ç‡: N/A (æ— æ–‡ä»¶)")
        
        if files_with_i18n:
            print(f"\nâœ… ä½¿ç”¨ useI18n çš„æ–‡ä»¶:")
            for file_path in files_with_i18n:
                print(f"  - {file_path}")
        
        if files_without_i18n:
            print(f"\nâš ï¸  æœªä½¿ç”¨ useI18n çš„æ–‡ä»¶:")
            for file_path in files_without_i18n:
                print(f"  - {file_path}")
                
                # æ£€æŸ¥è¿™äº›æ–‡ä»¶æ˜¯å¦æœ‰ç¡¬ç¼–ç å­—ç¬¦ä¸²
                chinese = find_chinese_strings(file_path)
                if chinese:
                    files_with_chinese.append((file_path, chinese))
                
                english = find_hardcoded_english(file_path)
                if english:
                    files_with_hardcoded_english.append((file_path, english))
    
    # 2. ç¡¬ç¼–ç å­—ç¬¦ä¸²æ£€æŸ¥
    print(f"\n\n{'=' * 80}")
    print("ç¡¬ç¼–ç å­—ç¬¦ä¸²æ£€æŸ¥")
    print(f"{'=' * 80}\n")
    
    if files_with_chinese:
        print(f"ğŸ”´ å‘ç°åŒ…å«ä¸­æ–‡å­—ç¬¦çš„æ–‡ä»¶ ({len(files_with_chinese)} ä¸ª):\n")
        for file_path, chinese_strings in files_with_chinese:
            print(f"  ğŸ“„ {file_path}")
            for line_num, line in chinese_strings[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"     L{line_num}: {line[:100]}")
            if len(chinese_strings) > 5:
                print(f"     ... è¿˜æœ‰ {len(chinese_strings) - 5} å¤„")
            print()
    else:
        print("âœ… æœªå‘ç°åŒ…å«ä¸­æ–‡å­—ç¬¦çš„ä»£ç ï¼ˆæ³¨é‡Šé™¤å¤–ï¼‰\n")
    
    if files_with_hardcoded_english:
        print(f"âš ï¸  å¯èƒ½åŒ…å«ç¡¬ç¼–ç è‹±æ–‡çš„æ–‡ä»¶ ({len(files_with_hardcoded_english)} ä¸ª):\n")
        for file_path, english_strings in files_with_hardcoded_english:
            print(f"  ğŸ“„ {file_path}")
            for line_num, line in english_strings[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"     L{line_num}: {line[:100]}")
            if len(english_strings) > 3:
                print(f"     ... è¿˜æœ‰ {len(english_strings) - 3} å¤„")
            print()
    
    # 3. i18n æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
    print(f"\n{'=' * 80}")
    print("i18n æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥")
    print(f"{'=' * 80}\n")
    
    zh_keys = load_i18n_keys('zh-CN')
    en_keys = load_i18n_keys('en')
    
    print(f"ğŸ“Š i18n Keys ç»Ÿè®¡:")
    print(f"  - ä¸­æ–‡ keys æ•°é‡: {len(zh_keys)}")
    print(f"  - è‹±æ–‡ keys æ•°é‡: {len(en_keys)}")
    
    comparison = compare_i18n_completeness(zh_keys, en_keys)
    
    if comparison['missing_in_en']:
        print(f"\nâš ï¸  è‹±æ–‡ç¿»è¯‘ç¼ºå¤±çš„ keys ({len(comparison['missing_in_en'])} ä¸ª):")
        for key in comparison['missing_in_en'][:10]:
            print(f"  - {key}")
        if len(comparison['missing_in_en']) > 10:
            print(f"  ... è¿˜æœ‰ {len(comparison['missing_in_en']) - 10} ä¸ª")
    
    if comparison['missing_in_zh']:
        print(f"\nâš ï¸  ä¸­æ–‡ç¿»è¯‘ç¼ºå¤±çš„ keys ({len(comparison['missing_in_zh'])} ä¸ª):")
        for key in comparison['missing_in_zh'][:10]:
            print(f"  - {key}")
        if len(comparison['missing_in_zh']) > 10:
            print(f"  ... è¿˜æœ‰ {len(comparison['missing_in_zh']) - 10} ä¸ª")
    
    if not comparison['missing_in_en'] and not comparison['missing_in_zh']:
        print("\nâœ… ä¸­è‹±æ–‡ç¿»è¯‘å®Œæ•´ï¼Œæ— ç¼ºå¤±")
    
    # 4. æ€»ç»“
    print(f"\n\n{'=' * 80}")
    print("æ£€æŸ¥æ€»ç»“")
    print(f"{'=' * 80}\n")
    
    print(f"ğŸ“Š æ•´ä½“ç»Ÿè®¡:")
    print(f"  - æ£€æŸ¥çš„ç»„ä»¶æ€»æ•°: {total_files}")
    if total_files > 0:
        print(f"  - ä½¿ç”¨ useI18n çš„ç»„ä»¶: {total_with_i18n} ({total_with_i18n / total_files * 100:.1f}%)")
        print(f"  - æœªä½¿ç”¨ useI18n çš„ç»„ä»¶: {total_without_i18n} ({total_without_i18n / total_files * 100:.1f}%)")
    else:
        print(f"  - ä½¿ç”¨ useI18n çš„ç»„ä»¶: {total_with_i18n}")
        print(f"  - æœªä½¿ç”¨ useI18n çš„ç»„ä»¶: {total_without_i18n}")
    print(f"  - åŒ…å«ä¸­æ–‡å­—ç¬¦çš„æ–‡ä»¶: {len(files_with_chinese)}")
    print(f"  - å¯èƒ½åŒ…å«ç¡¬ç¼–ç è‹±æ–‡çš„æ–‡ä»¶: {len(files_with_hardcoded_english)}")
    
    print(f"\nğŸ¯ å»ºè®®:")
    if total_without_i18n > 0:
        print(f"  1. ä¸º {total_without_i18n} ä¸ªæœªä½¿ç”¨ useI18n çš„ç»„ä»¶æ·»åŠ å›½é™…åŒ–æ”¯æŒ")
    if files_with_chinese:
        print(f"  2. å°† {len(files_with_chinese)} ä¸ªæ–‡ä»¶ä¸­çš„ä¸­æ–‡å­—ç¬¦ä¸²ç§»è‡³ i18n æ–‡ä»¶")
    if files_with_hardcoded_english:
        print(f"  3. æ£€æŸ¥ {len(files_with_hardcoded_english)} ä¸ªæ–‡ä»¶ä¸­çš„è‹±æ–‡å­—ç¬¦ä¸²æ˜¯å¦éœ€è¦å›½é™…åŒ–")
    if comparison['missing_in_en'] or comparison['missing_in_zh']:
        print(f"  4. è¡¥å……ç¼ºå¤±çš„ç¿»è¯‘ keys")
    
    if total_with_i18n == total_files and not files_with_chinese and not comparison['missing_in_en'] and not comparison['missing_in_zh']:
        print("  âœ… æ‰€æœ‰æ£€æŸ¥é¡¹å‡é€šè¿‡ï¼å›½é™…åŒ–å®ç°å®Œæ•´ã€‚")
    
    print(f"\n{'=' * 80}")

if __name__ == "__main__":
    generate_report()
