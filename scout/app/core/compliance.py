import re
from urllib.parse import urlparse
from urllib.robotparser import RobotFileParser
from loguru import logger
import asyncio

class ComplianceManager:
    """
    Scout çš„æ³•å¾‹ä¸åˆè§„å¼•æ“ã€‚
    ç¡®ä¿çˆ¬è™«æ°¸è¿œä¸è§¦ç¢° 'é«˜å‹çº¿'ã€‚
    """
    
    # 1. ç»å¯¹ç¦é£åŒº (æ”¿åºœã€å†›äº‹ã€æ‰§æ³•)
    DOMAIN_BLACKLIST = [
        ".gov.cn", ".gov",       # æ”¿åºœ
        ".mil.cn", ".mil",       # å†›äº‹
        ".police.cn",            # è­¦æ–¹
        ".politics",             # æ”¿æ²»ç›¸å…³é¡¶çº§åŸŸ
        "people.com.cn",         # é‡ç‚¹å®˜åª’
        "xinhuanet.com",
        "cctv.com",
        "81.cn",                 # ä¸­å›½å†›ç½‘
    ]
    
    # 2. URL æ•æ„Ÿå…³é”®è¯ (URL åŒ…å«å³æ‹¦æˆª)
    URL_SENSITIVE_KEYWORDS = [
        "zhengfu", "jiguan", "dangjian", # æ”¿åºœã€æœºå…³ã€å…šå»º
        "admin", "login", "signin",      # å°è¯•ç™»å½•/åå°
        "private", "internal",           # å†…éƒ¨ç³»ç»Ÿ
        "vpn.", "intranet."              # å†…ç½‘ç‰¹å¾
    ]

    # 3. å†…å®¹æ•æ„Ÿè¯åº“ (æ­£æ–‡åŒ…å«å³ç†”æ–­)
    # è¿™æ˜¯ä¸€ä¸ªåŸºç¡€åˆ—è¡¨ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ä»å¤–éƒ¨æ–‡ä»¶æˆ–é…ç½®åŠ è½½
    CONTENT_SENSITIVE_KEYWORDS = [
        # æ”¿æ²»æ•æ„Ÿç±» (ç¤ºä¾‹ï¼Œå®é™…éƒ¨ç½²éœ€æ‰©å……)
        "æ¶‰å¯†", "ç»å¯†", "æœºå¯†", "ç§˜å¯†çº§",
        "å†…éƒ¨èµ„æ–™", "ä¸¥ç¦å¤–ä¼ ",
        "è‰²æƒ…", "èµŒåš", "åšå½©", # åŸºç¡€é»„èµŒæ¯’
        # "ååŠ¨", "æš´æ" ç­‰å…·ä½“è¯æ±‡æ ¹æ®å®é™…åˆè§„è¦æ±‚æ·»åŠ 
    ]

    def __init__(self, user_agent: str = "*"):
        self.user_agent = user_agent
        self._robots_cache = {} # ç®€å•å†…å­˜ç¼“å­˜

    async def is_safe_to_crawl(self, url: str) -> bool:
        """
        [Pre-Flight] é£è¡Œå‰æ£€æŸ¥ï¼š
        1. æ£€æŸ¥é»‘åå• (æ”¿æ²»å®‰å…¨)
        2. æ£€æŸ¥ Robots.txt (æ³•å¾‹åˆè§„)
        """
        if not self._check_blacklist(url):
            logger.warning(f"ğŸš« Compliance Block (Blacklist URL): {url}")
            return False
            
        # 3. ä¸¥æ ¼éµå¾ª Robots åè®®
        if not await self._check_robots_txt(url):
            logger.warning(f"ğŸš« Compliance Block (Robots.txt): {url}")
            return False
            
        return True

    def is_content_safe(self, text: str) -> bool:
        """
        [Post-Flight] è½åœ°åå®‰æ£€ï¼š
        æ£€æŸ¥æŠ“å–å›æ¥çš„ Markdown å†…å®¹æ˜¯å¦åŒ…å«æ•æ„Ÿè¯ã€‚
        """
        if not text:
            return True
            
        # ç®€å•é«˜æ•ˆçš„å­—ç¬¦ä¸²åŒ¹é…ï¼Œé‡å¤§æ—¶å¯è€ƒè™‘ Aho-Corasick ç®—æ³•ä¼˜åŒ–
        for kw in self.CONTENT_SENSITIVE_KEYWORDS:
            if kw in text:
                logger.warning(f"ğŸš« Compliance Block (Sensitive Content): Found keyword '{kw}'")
                return False
        
        return True

    def _check_blacklist(self, url: str) -> bool:
        try:
            parsed = urlparse(url)
            hostname = parsed.netloc.lower()
            path = parsed.path.lower()
            
            # æ£€æŸ¥åŸŸååç¼€
            for domain in self.DOMAIN_BLACKLIST:
                if hostname.endswith(domain) or f"{domain}." in hostname:
                    return False
                    
            # æ£€æŸ¥æ•æ„Ÿè¯
            full_str = f"{hostname}{path}"
            for kw in self.URL_SENSITIVE_KEYWORDS:
                if kw in full_str:
                    return False
            return True
        except Exception:
            return False

    async def _check_robots_txt(self, url: str) -> bool:
        """
        å¼‚æ­¥æ£€æŸ¥ Robots.txt
        """
        try:
            parsed = urlparse(url)
            robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
            
            if robots_url in self._robots_cache:
                rp = self._robots_cache[robots_url]
            else:
                rp = RobotFileParser()
                # ä½¿ç”¨ run_in_executor é¿å…é˜»å¡å¼‚æ­¥å¾ªç¯
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(None, rp.set_url, robots_url)
                # è®¾ç½®è¶…æ—¶ï¼Œé˜²æ­¢ robots.txt åŠ è½½å¡æ­»
                await asyncio.wait_for(loop.run_in_executor(None, rp.read), timeout=5.0)
                self._robots_cache[robots_url] = rp
            
            return rp.can_fetch(self.user_agent, url)
        except Exception:
            # è·å– robots.txt å¤±è´¥æˆ–è¶…æ—¶ï¼Œé»˜è®¤å…è®¸ (å®½æ¾æ¨¡å¼)
            # æˆ–è€…ä¸ºäº†æè‡´åˆè§„ï¼Œè¿™é‡Œå¯ä»¥ return False
            return True

# å…¨å±€åˆè§„å®ä¾‹
compliance_manager = ComplianceManager(user_agent="DeetingScout/1.0")