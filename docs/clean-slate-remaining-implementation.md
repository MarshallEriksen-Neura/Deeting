# å‰©ä½™åŠŸèƒ½å®ç°æ–¹æ¡ˆ

## ğŸ¯ å®ç°ç›®æ ‡

å®Œæˆä»¥ä¸‹æœªå®ç°çš„åŠŸèƒ½ï¼š
1. è°ƒæ•´æµç¨‹ï¼šquota_check æ‰£å‡ï¼Œbilling åªå†™æµæ°´
2. å‘¨æœŸæ€§åŒæ­¥ä»»åŠ¡ï¼šRedis â†’ DB
3. API Key é¢„ç®—ï¼šRedis Hash + å®šæ—¶åŒæ­¥
4. äº‹åŠ¡åè°ƒåº¦å™¨ï¼šTransactionAwareCelery
5. ä¼šè¯åˆ†å¸ƒå¼é”
6. è·¯ç”±äº²å’ŒçŠ¶æ€æœº
7. ç±»å‹åŒ– Context
8. ç¼“å­˜é¢„çƒ­ SETNX

---

## 1ï¸âƒ£ è°ƒæ•´æµç¨‹ï¼šquota_check æ‰£å‡ï¼Œbilling åªå†™æµæ°´

### è®¾è®¡æ€è·¯

**å½“å‰é—®é¢˜**ï¼š
- quota_check åªæ£€æŸ¥ä¸æ‰£å‡
- billing æ­¥éª¤æ‰£å‡é…é¢
- å¯¼è‡´æ£€æŸ¥å’Œæ‰£å‡åˆ†ç¦»ï¼Œå¯èƒ½å‡ºç°å¹¶å‘é—®é¢˜

**æ–°æ–¹æ¡ˆ**ï¼š
- quota_check æ­¥éª¤ï¼šè°ƒç”¨ quota_deduct.lua åŸå­æ‰£å‡é…é¢
- billing æ­¥éª¤ï¼šåªå†™äº¤æ˜“æµæ°´ï¼Œä¸æ‰£å‡é…é¢
- ä¼˜ç‚¹ï¼šåŸå­æ€§æ›´å¼ºï¼Œé€»è¾‘æ›´æ¸…æ™°

### å®ç°ä»£ç 

#### 1.1 ä¿®æ”¹ QuotaCheckStepï¼ˆæ‰£å‡é…é¢ï¼‰

```python
# backend/app/services/workflow/steps/quota_check.py

@step_registry.register
class QuotaCheckStep(BaseStep):
    """
    é…é¢æ£€æŸ¥æ­¥éª¤ï¼ˆæ£€æŸ¥ + æ‰£å‡ï¼‰
    
    å˜æ›´ï¼š
    - ä¸å†åªæ£€æŸ¥ï¼Œè€Œæ˜¯ç›´æ¥æ‰£å‡é…é¢
    - ä½¿ç”¨ Redis Lua è„šæœ¬åŸå­æ‰£å‡
    - æ‰£å‡æˆåŠŸåå†™å…¥ Context ä¾› billing ä½¿ç”¨
    """
    
    name = "quota_check"
    depends_on = ["validation"]
    
    async def execute(self, ctx: "WorkflowContext") -> StepResult:
        """æ‰§è¡Œé…é¢æ£€æŸ¥å¹¶æ‰£å‡"""
        tenant_id = ctx.tenant_id
        
        if not tenant_id:
            if ctx.is_external:
                ctx.mark_error(
                    ErrorSource.GATEWAY,
                    "QUOTA_NO_TENANT",
                    "Tenant required for external requests",
                )
                return StepResult(status=StepStatus.FAILED)
            return StepResult(status=StepStatus.SUCCESS)
        
        # ä¼°ç®—è´¹ç”¨ï¼ˆç”¨äºé¢„æ‰£å‡ï¼‰
        estimated_cost = await self._estimate_cost(ctx)
        
        try:
            # è°ƒç”¨ Redis Lua è„šæœ¬åŸå­æ‰£å‡é…é¢
            quota_info = await self._deduct_quota_redis(
                ctx, str(tenant_id), estimated_cost
            )
            
            # å†™å…¥ä¸Šä¸‹æ–‡ä¾› billing ä½¿ç”¨
            ctx.set("quota_check", "balance_before", quota_info["balance_before"])
            ctx.set("quota_check", "balance_after", quota_info["balance_after"])
            ctx.set("quota_check", "daily_used", quota_info["daily_used"])
            ctx.set("quota_check", "monthly_used", quota_info["monthly_used"])
            ctx.set("quota_check", "estimated_cost", estimated_cost)
            
            logger.info(
                f"Quota deducted trace_id={ctx.trace_id} "
                f"estimated_cost={estimated_cost:.6f} "
                f"balance_after={quota_info['balance_after']:.2f}"
            )
            
            return StepResult(status=StepStatus.SUCCESS, data=quota_info)
            
        except QuotaExceededError as e:
            logger.warning(f"Quota exceeded: {e}")
            ctx.mark_error(
                ErrorSource.GATEWAY,
                f"QUOTA_{e.quota_type.upper()}_EXCEEDED",
                str(e),
            )
            return StepResult(status=StepStatus.FAILED, message=str(e))
    
    async def _deduct_quota_redis(
        self,
        ctx: "WorkflowContext",
        tenant_id: str,
        estimated_cost: float,
    ) -> dict:
        """
        ä½¿ç”¨ Redis Lua è„šæœ¬åŸå­æ‰£å‡é…é¢
        
        æµç¨‹ï¼š
        1. æ£€æŸ¥ Redis Hash æ˜¯å¦å­˜åœ¨
        2. ä¸å­˜åœ¨åˆ™ä» DB é¢„çƒ­
        3. è°ƒç”¨ quota_deduct.lua è„šæœ¬æ‰£å‡
        4. è¿”å›æ‰£å‡åçš„é…é¢ä¿¡æ¯
        """
        redis_client = getattr(cache, "_redis", None)
        if not redis_client:
            # Redis ä¸å¯ç”¨ï¼Œå›é€€åˆ° DB
            return await self._deduct_quota_db(ctx, tenant_id, estimated_cost)
        
        # åŠ è½½ Lua è„šæœ¬
        script_sha = cache.get_script_sha("quota_deduct")
        if not script_sha:
            await cache.preload_scripts()
            script_sha = cache.get_script_sha("quota_deduct")
        
        if not script_sha:
            # è„šæœ¬åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ° DB
            return await self._deduct_quota_db(ctx, tenant_id, estimated_cost)
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
        key = CacheKeys.quota_hash(tenant_id)
        exists = await redis_client.exists(cache._make_key(key))
        
        if not exists:
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œä» DB é¢„çƒ­ï¼ˆä½¿ç”¨ SETNX é˜²æ­¢ç«æ€ï¼‰
            await self._warm_quota_cache_safe(ctx, redis_client, key, tenant_id)
        
        # è°ƒç”¨ Lua è„šæœ¬æ‰£å‡é…é¢
        today = self._today_str()
        month = self._month_str()
        
        result = await redis_client.evalsha(
            script_sha,
            keys=[cache._make_key(key)],
            args=[
                str(estimated_cost),  # amount
                "1",  # daily_requests
                "1",  # monthly_requests
                today,
                month,
                "0",  # allow_negative (é¢„æ‰£å‡ä¸å…è®¸è´Ÿå€¼)
            ]
        )
        
        # è§£æç»“æœ
        # result: [success, message, new_balance, new_daily_used, new_monthly_used, version]
        if result[0] == 0:
            # æ‰£å‡å¤±è´¥
            error_type = result[1]
            if error_type == "INSUFFICIENT_BALANCE":
                raise QuotaExceededError("balance", float(result[2]), float(result[4]))
            elif error_type == "DAILY_QUOTA_EXCEEDED":
                raise QuotaExceededError("daily", float(result[2]), float(result[3]))
            elif error_type == "MONTHLY_QUOTA_EXCEEDED":
                raise QuotaExceededError("monthly", float(result[2]), float(result[3]))
            else:
                raise QuotaExceededError("unknown", 0, 0)
        
        # æ‰£å‡æˆåŠŸ
        balance_before = float(result[2]) + estimated_cost  # åæ¨æ‰£å‡å‰ä½™é¢
        
        return {
            "balance_before": balance_before,
            "balance_after": float(result[2]),
            "daily_used": int(result[3]),
            "monthly_used": int(result[4]),
            "version": int(result[5]),
        }
    
    async def _warm_quota_cache_safe(
        self,
        ctx: "WorkflowContext",
        redis_client,
        cache_key: str,
        tenant_id: str,
    ) -> None:
        """
        ä» DB é¢„çƒ­é…é¢ç¼“å­˜ï¼ˆä½¿ç”¨ SETNX é˜²æ­¢ç«æ€ï¼‰
        
        æµç¨‹ï¼š
        1. ä½¿ç”¨ SETNX è®¾ç½®é¢„çƒ­é”
        2. å¦‚æœé”è®¾ç½®æˆåŠŸï¼Œä» DB è¯»å–å¹¶å†™å…¥ Redis
        3. å¦‚æœé”è®¾ç½®å¤±è´¥ï¼Œç­‰å¾…å…¶ä»–è¿›ç¨‹é¢„çƒ­å®Œæˆ
        """
        lock_key = f"{cache_key}:warming"
        
        # å°è¯•è·å–é¢„çƒ­é”ï¼ˆ30 ç§’è¿‡æœŸï¼‰
        locked = await redis_client.set(
            cache._make_key(lock_key),
            "1",
            ex=30,
            nx=True
        )
        
        if locked:
            # è·å–é”æˆåŠŸï¼Œæ‰§è¡Œé¢„çƒ­
            try:
                repo = QuotaRepository(ctx.db_session)
                quota = await repo.get_or_create(tenant_id)
                
                payload = {
                    "balance": str(quota.balance),
                    "credit_limit": str(quota.credit_limit),
                    "daily_quota": str(quota.daily_quota),
                    "daily_used": str(quota.daily_used),
                    "daily_date": quota.daily_reset_at.isoformat() if quota.daily_reset_at else self._today_str(),
                    "monthly_quota": str(quota.monthly_quota),
                    "monthly_used": str(quota.monthly_used),
                    "monthly_month": quota.monthly_reset_at.strftime("%Y-%m") if quota.monthly_reset_at else self._month_str(),
                    "rpm_limit": str(quota.rpm_limit) if quota.rpm_limit else "0",
                    "tpm_limit": str(quota.tpm_limit) if quota.tpm_limit else "0",
                    "version": str(quota.version),
                }
                
                await redis_client.hset(cache._make_key(cache_key), mapping=payload)
                await redis_client.expire(cache._make_key(cache_key), 86400)
                
                logger.info(f"Warmed quota cache for tenant={tenant_id}")
            finally:
                # é‡Šæ”¾é”
                await redis_client.delete(cache._make_key(lock_key))
        else:
            # è·å–é”å¤±è´¥ï¼Œç­‰å¾…å…¶ä»–è¿›ç¨‹é¢„çƒ­å®Œæˆ
            for _ in range(10):  # æœ€å¤šç­‰å¾… 1 ç§’
                await asyncio.sleep(0.1)
                exists = await redis_client.exists(cache._make_key(cache_key))
                if exists:
                    return
            
            # è¶…æ—¶åä»æœªé¢„çƒ­ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise Exception(f"Quota cache warming timeout for tenant={tenant_id}")
```



#### 1.2 ä¿®æ”¹ BillingStepï¼ˆåªå†™æµæ°´ï¼‰

```python
# backend/app/services/workflow/steps/billing.py

@step_registry.register
class BillingStep(BaseStep):
    """
    è®¡è´¹æ­¥éª¤ï¼ˆåªå†™æµæ°´ï¼Œä¸æ‰£å‡é…é¢ï¼‰
    
    å˜æ›´ï¼š
    - ä¸å†æ‰£å‡é…é¢ï¼ˆå·²åœ¨ quota_check æ‰£å‡ï¼‰
    - åªåˆ›å»ºäº¤æ˜“æµæ°´è®°å½•
    - è®¡ç®—å®é™…è´¹ç”¨å¹¶è°ƒæ•´ä½™é¢ï¼ˆå¦‚æœé¢„æ‰£å‡ä¸å‡†ç¡®ï¼‰
    """
    
    name = "billing"
    depends_on = ["response_transform"]
    
    async def execute(self, ctx: "WorkflowContext") -> StepResult:
        """æ‰§è¡Œè®¡è´¹ï¼ˆåªå†™æµæ°´ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦æµå¼
        is_stream = ctx.get("upstream_call", "stream", False)
        
        if is_stream:
            # æµå¼ï¼šåˆ›å»º PENDING äº¤æ˜“
            return await self._create_pending_for_stream(ctx)
        else:
            # éæµå¼ï¼šå†™äº¤æ˜“æµæ°´
            return await self._record_transaction(ctx)
    
    async def _record_transaction(self, ctx: "WorkflowContext") -> StepResult:
        """
        è®°å½•äº¤æ˜“æµæ°´ï¼ˆéæµå¼ï¼‰
        
        æµç¨‹ï¼š
        1. è®¡ç®—å®é™…è´¹ç”¨
        2. ä» Context è·å–é¢„æ‰£å‡ä¿¡æ¯
        3. åˆ›å»ºäº¤æ˜“è®°å½•
        4. å¦‚æœå®é™…è´¹ç”¨ä¸é¢„ä¼°ä¸åŒï¼Œè°ƒæ•´ä½™é¢
        """
        # è·å– token ç”¨é‡
        input_tokens = ctx.billing.input_tokens
        output_tokens = ctx.billing.output_tokens
        
        # è·å–å®šä»·é…ç½®
        pricing = ctx.get("routing", "pricing_config") or {}
        
        if not pricing or not ctx.is_external or not ctx.tenant_id:
            # æ— éœ€è®¡è´¹
            ctx.set("billing", "skip_reason", "no_pricing_or_internal")
            return StepResult(status=StepStatus.SUCCESS)
        
        # è®¡ç®—å®é™…è´¹ç”¨
        input_cost = self._calculate_cost(input_tokens, pricing.get("input_per_1k", 0))
        output_cost = self._calculate_cost(output_tokens, pricing.get("output_per_1k", 0))
        actual_cost = input_cost + output_cost
        currency = pricing.get("currency", "USD")
        
        # æ›´æ–° billing ä¿¡æ¯
        ctx.billing.input_cost = input_cost
        ctx.billing.output_cost = output_cost
        ctx.billing.total_cost = actual_cost
        ctx.billing.currency = currency
        
        # è·å–é¢„æ‰£å‡ä¿¡æ¯
        estimated_cost = ctx.get("quota_check", "estimated_cost") or 0.0
        balance_before = ctx.get("quota_check", "balance_before") or 0.0
        balance_after = ctx.get("quota_check", "balance_after") or 0.0
        
        # è®¡ç®—å·®é¢
        cost_diff = actual_cost - estimated_cost
        
        # åˆ›å»ºäº¤æ˜“è®°å½•
        try:
            repo = BillingRepository(ctx.db_session)
            transaction = await repo.record_transaction(
                tenant_id=ctx.tenant_id,
                trace_id=ctx.trace_id,
                amount=Decimal(str(actual_cost)),
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                input_price=Decimal(str(pricing.get("input_per_1k", 0))),
                output_price=Decimal(str(pricing.get("output_per_1k", 0))),
                balance_before=Decimal(str(balance_before)),
                balance_after=Decimal(str(balance_after - cost_diff)),  # è°ƒæ•´åçš„ä½™é¢
                provider=ctx.upstream_result.provider,
                model=ctx.requested_model,
                preset_item_id=ctx.get("routing", "provider_model_id"),
                api_key_id=ctx.api_key_id,
                cost_diff=Decimal(str(cost_diff)),  # è®°å½•å·®é¢
            )
            
            # å¦‚æœæœ‰å·®é¢ï¼Œè°ƒæ•´ Redis ä½™é¢
            if abs(cost_diff) > 0.000001:
                await self._adjust_redis_balance(ctx, cost_diff)
            
            ctx.set("billing", "balance_after", float(transaction.balance_after))
            
            logger.info(
                f"Billing recorded trace_id={ctx.trace_id} "
                f"actual_cost={actual_cost:.6f} "
                f"estimated_cost={estimated_cost:.6f} "
                f"diff={cost_diff:.6f}"
            )
            
            return StepResult(
                status=StepStatus.SUCCESS,
                data={
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_cost": actual_cost,
                    "currency": currency,
                    "cost_diff": cost_diff,
                },
            )
            
        except Exception as e:
            logger.error(f"Billing record failed: {e}")
            ctx.mark_error(
                ErrorSource.GATEWAY,
                "BILLING_RECORD_FAILED",
                str(e),
            )
            return StepResult(status=StepStatus.FAILED, message=str(e))
    
    async def _adjust_redis_balance(self, ctx: "WorkflowContext", cost_diff: float) -> None:
        """
        è°ƒæ•´ Redis ä½™é¢ï¼ˆå¦‚æœå®é™…è´¹ç”¨ä¸é¢„ä¼°ä¸åŒï¼‰
        
        æµç¨‹ï¼š
        1. è°ƒç”¨ Redis HINCRBY è°ƒæ•´ä½™é¢
        2. è®°å½•è°ƒæ•´æ—¥å¿—
        """
        redis_client = getattr(cache, "_redis", None)
        if not redis_client:
            return
        
        try:
            key = CacheKeys.quota_hash(str(ctx.tenant_id))
            
            # è°ƒæ•´ä½™é¢ï¼ˆæ³¨æ„ï¼šcost_diff ä¸ºæ­£è¡¨ç¤ºå®é™…è´¹ç”¨æ›´é«˜ï¼Œéœ€è¦å†æ‰£å‡ï¼‰
            await redis_client.hincrbyfloat(
                cache._make_key(key),
                "balance",
                -cost_diff  # è´Ÿæ•°è¡¨ç¤ºæ‰£å‡
            )
            
            logger.info(
                f"Adjusted Redis balance trace_id={ctx.trace_id} "
                f"diff={cost_diff:.6f}"
            )
        except Exception as e:
            logger.error(f"Failed to adjust Redis balance: {e}")
```



---

## 2ï¸âƒ£ å‘¨æœŸæ€§åŒæ­¥ä»»åŠ¡ï¼šRedis â†’ DB

### è®¾è®¡æ€è·¯

**ç›®æ ‡**ï¼š
- Redis ä½œä¸ºé…é¢çš„å•ä¸€çœŸæºï¼ˆå®æ—¶æ‰£å‡ï¼‰
- DB ä½œä¸ºæŒä¹…åŒ–å’Œå®¡è®¡ï¼ˆå‘¨æœŸæ€§åŒæ­¥ï¼‰
- å®šæ—¶ä»»åŠ¡å°† Redis æ•°æ®åŒæ­¥åˆ° DB

### å®ç°ä»£ç 

#### 2.1 åˆ›å»ºåŒæ­¥ä»»åŠ¡

```python
# backend/app/tasks/quota_sync.py

"""
é…é¢åŒæ­¥ä»»åŠ¡ï¼šRedis â†’ DB

èŒè´£ï¼š
- å®šæœŸå°† Redis ä¸­çš„é…é¢æ•°æ®åŒæ­¥åˆ° DB
- ç”¨äºå®¡è®¡å’ŒæŒä¹…åŒ–
- æ£€æµ‹å¹¶ä¿®å¤ä¸ä¸€è‡´
"""

import logging
from datetime import datetime
from decimal import Decimal
from typing import List

from celery import shared_task
from sqlalchemy import select
from sqlalchemy.orm import Session

from app.core.cache import cache
from app.core.cache_keys import CacheKeys
from app.core.database import get_sync_session
from app.models.billing import TenantQuota
from app.repositories.quota_repository import QuotaRepository

logger = logging.getLogger(__name__)


@shared_task(name="quota_sync.sync_all_tenants")
def sync_all_tenants_quota():
    """
    åŒæ­¥æ‰€æœ‰ç§Ÿæˆ·çš„é…é¢ï¼ˆRedis â†’ DBï¼‰
    
    æ‰§è¡Œé¢‘ç‡ï¼šæ¯ 5 åˆ†é’Ÿ
    """
    logger.info("Starting quota sync task")
    
    with get_sync_session() as session:
        # è·å–æ‰€æœ‰ç§Ÿæˆ·
        stmt = select(TenantQuota.tenant_id).distinct()
        result = session.execute(stmt)
        tenant_ids = [str(row[0]) for row in result.fetchall()]
        
        success_count = 0
        failure_count = 0
        
        for tenant_id in tenant_ids:
            try:
                sync_tenant_quota(tenant_id, session)
                success_count += 1
            except Exception as e:
                logger.error(f"Failed to sync quota for tenant={tenant_id}: {e}")
                failure_count += 1
        
        logger.info(
            f"Quota sync completed: success={success_count}, failure={failure_count}"
        )
    
    return {
        "success_count": success_count,
        "failure_count": failure_count,
        "total": len(tenant_ids),
    }


def sync_tenant_quota(tenant_id: str, session: Session) -> None:
    """
    åŒæ­¥å•ä¸ªç§Ÿæˆ·çš„é…é¢ï¼ˆRedis â†’ DBï¼‰
    
    æµç¨‹ï¼š
    1. ä» Redis è¯»å–é…é¢æ•°æ®
    2. ä» DB è¯»å–é…é¢æ•°æ®
    3. æ¯”è¾ƒå·®å¼‚
    4. æ›´æ–° DBï¼ˆä»¥ Redis ä¸ºå‡†ï¼‰
    """
    redis_client = getattr(cache, "_redis_sync", None)
    if not redis_client:
        logger.warning("Redis not available, skip sync")
        return
    
    # ä» Redis è¯»å–é…é¢
    key = CacheKeys.quota_hash(tenant_id)
    redis_data = redis_client.hgetall(cache._make_key(key))
    
    if not redis_data:
        logger.debug(f"No Redis data for tenant={tenant_id}, skip")
        return
    
    # è§£æ Redis æ•°æ®
    redis_quota = {
        "balance": Decimal(redis_data.get(b"balance", b"0").decode()),
        "credit_limit": Decimal(redis_data.get(b"credit_limit", b"0").decode()),
        "daily_quota": int(redis_data.get(b"daily_quota", b"0").decode()),
        "daily_used": int(redis_data.get(b"daily_used", b"0").decode()),
        "daily_date": redis_data.get(b"daily_date", b"").decode(),
        "monthly_quota": int(redis_data.get(b"monthly_quota", b"0").decode()),
        "monthly_used": int(redis_data.get(b"monthly_used", b"0").decode()),
        "monthly_month": redis_data.get(b"monthly_month", b"").decode(),
        "version": int(redis_data.get(b"version", b"0").decode()),
    }
    
    # ä» DB è¯»å–é…é¢
    repo = QuotaRepository(session)
    db_quota = repo.get_or_create_sync(tenant_id)
    
    # æ¯”è¾ƒå·®å¼‚
    diff = {
        "balance": float(redis_quota["balance"] - db_quota.balance),
        "daily_used": redis_quota["daily_used"] - db_quota.daily_used,
        "monthly_used": redis_quota["monthly_used"] - db_quota.monthly_used,
    }
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚
    has_diff = (
        abs(diff["balance"]) > 0.01 or
        diff["daily_used"] != 0 or
        diff["monthly_used"] != 0
    )
    
    if has_diff:
        logger.warning(
            f"Quota diff detected for tenant={tenant_id}: "
            f"balance_diff={diff['balance']:.6f}, "
            f"daily_diff={diff['daily_used']}, "
            f"monthly_diff={diff['monthly_used']}"
        )
        
        # æ›´æ–° DBï¼ˆä»¥ Redis ä¸ºå‡†ï¼‰
        db_quota.balance = redis_quota["balance"]
        db_quota.daily_used = redis_quota["daily_used"]
        db_quota.monthly_used = redis_quota["monthly_used"]
        db_quota.version = redis_quota["version"]
        
        session.commit()
        
        logger.info(f"Synced quota for tenant={tenant_id}")
        
        # å‘é€å‘Šè­¦ï¼ˆå¦‚æœå·®å¼‚è¿‡å¤§ï¼‰
        if abs(diff["balance"]) > 1.0:
            from app.core.monitoring import alert_quota_diff
            alert_quota_diff(tenant_id, diff)
    else:
        logger.debug(f"No diff for tenant={tenant_id}, skip")


@shared_task(name="quota_sync.check_consistency")
def check_quota_consistency():
    """
    æ£€æŸ¥é…é¢ä¸€è‡´æ€§ï¼ˆRedis vs DBï¼‰
    
    æ‰§è¡Œé¢‘ç‡ï¼šæ¯ 1 å°æ—¶
    """
    logger.info("Starting quota consistency check")
    
    with get_sync_session() as session:
        # è·å–æ‰€æœ‰ç§Ÿæˆ·
        stmt = select(TenantQuota)
        result = session.execute(stmt)
        quotas = result.scalars().all()
        
        inconsistent_count = 0
        
        for quota in quotas:
            tenant_id = str(quota.tenant_id)
            
            # ä» Redis è¯»å–
            redis_client = getattr(cache, "_redis_sync", None)
            if not redis_client:
                continue
            
            key = CacheKeys.quota_hash(tenant_id)
            redis_data = redis_client.hgetall(cache._make_key(key))
            
            if not redis_data:
                continue
            
            # æ¯”è¾ƒä½™é¢
            redis_balance = Decimal(redis_data.get(b"balance", b"0").decode())
            db_balance = quota.balance
            
            diff = float(redis_balance - db_balance)
            
            if abs(diff) > 0.01:
                inconsistent_count += 1
                logger.warning(
                    f"Inconsistent quota: tenant={tenant_id}, "
                    f"redis_balance={redis_balance}, "
                    f"db_balance={db_balance}, "
                    f"diff={diff:.6f}"
                )
                
                # è®°å½•åˆ°ç›‘æ§
                from app.core.monitoring import quota_redis_db_diff
                quota_redis_db_diff.labels(tenant_id=tenant_id).set(abs(diff))
        
        logger.info(
            f"Consistency check completed: inconsistent={inconsistent_count}/{len(quotas)}"
        )
    
    return {
        "total": len(quotas),
        "inconsistent": inconsistent_count,
    }
```

#### 2.2 é…ç½® Celery Beat å®šæ—¶ä»»åŠ¡

```python
# backend/app/core/celery_app.py

from celery import Celery
from celery.schedules import crontab

celery_app = Celery("apiproxy")

# Celery Beat å®šæ—¶ä»»åŠ¡é…ç½®
celery_app.conf.beat_schedule = {
    # æ¯ 5 åˆ†é’ŸåŒæ­¥é…é¢
    "sync-quota-every-5-minutes": {
        "task": "quota_sync.sync_all_tenants",
        "schedule": crontab(minute="*/5"),
    },
    # æ¯ 1 å°æ—¶æ£€æŸ¥ä¸€è‡´æ€§
    "check-quota-consistency-hourly": {
        "task": "quota_sync.check_consistency",
        "schedule": crontab(minute=0),
    },
}
```

---

## 3ï¸âƒ£ API Key é¢„ç®—ï¼šRedis Hash + å®šæ—¶åŒæ­¥

### è®¾è®¡æ€è·¯

**ç›®æ ‡**ï¼š
- API Key çš„ budget_used å­˜å‚¨åœ¨ Redis Hash ä¸­
- quota_check å’Œ billing æ­¥éª¤æ›´æ–° Redis
- å®šæ—¶ä»»åŠ¡åŒæ­¥åˆ° DB

### å®ç°ä»£ç 

#### 3.1 åˆ›å»º API Key é…é¢ Lua è„šæœ¬

```lua
-- backend/app/core/redis_scripts/apikey_budget_deduct.lua

-- KEYS[1]: gw:quota:apikey:{api_key_id}
-- ARGV[1]: amount (æ‰£å‡é‡‘é¢)
-- ARGV[2]: budget_limit (é¢„ç®—ä¸Šé™)

local key = KEYS[1]

-- æ£€æŸ¥ Hash æ˜¯å¦å­˜åœ¨
if redis.call('EXISTS', key) == 0 then
    return {0, 'APIKEY_NOT_FOUND'}
end

-- è¯»å–å½“å‰ç”¨é‡
local budget_used = tonumber(redis.call('HGET', key, 'budget_used') or 0)
local budget_limit = tonumber(ARGV[2])
local amount = tonumber(ARGV[1])

-- æ£€æŸ¥æ˜¯å¦è¶…é™
local new_budget_used = budget_used + amount
if budget_limit > 0 and new_budget_used > budget_limit then
    return {0, 'BUDGET_EXCEEDED', budget_limit, budget_used}
end

-- æ‰£å‡
redis.call('HINCRBYFLOAT', key, 'budget_used', amount)

-- æ›´æ–°æ—¶é—´æˆ³
redis.call('HSET', key, 'updated_at', ARGV[3] or '')

return {1, 'OK', new_budget_used}
```

#### 3.2 ä¿®æ”¹ QuotaCheckStepï¼ˆæ£€æŸ¥ API Key é¢„ç®—ï¼‰

```python
# backend/app/services/workflow/steps/quota_check.py

async def _check_apikey_budget(
    self,
    ctx: "WorkflowContext",
    api_key_id: str,
    estimated_cost: float,
) -> None:
    """
    æ£€æŸ¥ API Key é¢„ç®—
    
    æµç¨‹ï¼š
    1. ä» Context è·å– budget_limit
    2. ä» Redis è¯»å– budget_used
    3. æ£€æŸ¥æ˜¯å¦è¶…é™
    """
    budget_limit = ctx.get("external_auth", "budget_limit")
    if budget_limit is None or budget_limit <= 0:
        # æ— é¢„ç®—é™åˆ¶
        return
    
    redis_client = getattr(cache, "_redis", None)
    if not redis_client:
        # Redis ä¸å¯ç”¨ï¼Œå›é€€åˆ° DB
        return await self._check_apikey_budget_db(ctx, api_key_id, budget_limit)
    
    # ä» Redis è¯»å– budget_used
    key = CacheKeys.apikey_budget_hash(api_key_id)
    exists = await redis_client.exists(cache._make_key(key))
    
    if not exists:
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œä» DB é¢„çƒ­
        await self._warm_apikey_budget_cache(ctx, redis_client, key, api_key_id)
    
    budget_used = await redis_client.hget(cache._make_key(key), "budget_used")
    budget_used = float(budget_used) if budget_used else 0.0
    
    # æ£€æŸ¥æ˜¯å¦è¶…é™
    if budget_used + estimated_cost > budget_limit:
        raise QuotaExceededError("budget", budget_limit, budget_used)
    
    logger.debug(
        f"API Key budget check passed: api_key_id={api_key_id}, "
        f"budget_used={budget_used:.2f}, budget_limit={budget_limit:.2f}"
    )
```

#### 3.3 ä¿®æ”¹ BillingStepï¼ˆæ›´æ–° API Key é¢„ç®—ï¼‰

```python
# backend/app/services/workflow/steps/billing.py

async def _update_apikey_budget(
    self,
    ctx: "WorkflowContext",
    actual_cost: float,
) -> None:
    """
    æ›´æ–° API Key é¢„ç®—ï¼ˆRedisï¼‰
    
    æµç¨‹ï¼š
    1. è°ƒç”¨ Redis HINCRBYFLOAT æ›´æ–° budget_used
    2. è®°å½•æ›´æ–°æ—¥å¿—
    """
    api_key_id = ctx.api_key_id
    if not api_key_id:
        return
    
    budget_limit = ctx.get("external_auth", "budget_limit")
    if budget_limit is None or budget_limit <= 0:
        # æ— é¢„ç®—é™åˆ¶
        return
    
    redis_client = getattr(cache, "_redis", None)
    if not redis_client:
        return
    
    try:
        key = CacheKeys.apikey_budget_hash(str(api_key_id))
        
        # æ›´æ–° budget_used
        new_budget_used = await redis_client.hincrbyfloat(
            cache._make_key(key),
            "budget_used",
            actual_cost
        )
        
        logger.info(
            f"Updated API Key budget: api_key_id={api_key_id}, "
            f"cost={actual_cost:.6f}, new_budget_used={new_budget_used:.6f}"
        )
        
        # æ£€æŸ¥æ˜¯å¦æ¥è¿‘ä¸Šé™ï¼ˆå‘é€å‘Šè­¦ï¼‰
        if budget_limit > 0 and new_budget_used > budget_limit * 0.9:
            from app.core.monitoring import alert_apikey_budget_warning
            alert_apikey_budget_warning(str(api_key_id), new_budget_used, budget_limit)
        
    except Exception as e:
        logger.error(f"Failed to update API Key budget: {e}")
```

#### 3.4 åˆ›å»º API Key é¢„ç®—åŒæ­¥ä»»åŠ¡

```python
# backend/app/tasks/apikey_sync.py

"""
API Key é¢„ç®—åŒæ­¥ä»»åŠ¡ï¼šRedis â†’ DB

èŒè´£ï¼š
- å®šæœŸå°† Redis ä¸­çš„ API Key é¢„ç®—æ•°æ®åŒæ­¥åˆ° DB
- ç”¨äºå®¡è®¡å’ŒæŒä¹…åŒ–
"""

import logging
from decimal import Decimal

from celery import shared_task
from sqlalchemy import select
from sqlalchemy.orm import Session

from app.core.cache import cache
from app.core.cache_keys import CacheKeys
from app.core.database import get_sync_session
from app.models.api_key import ApiKey, ApiKeyQuota, QuotaType

logger = logging.getLogger(__name__)


@shared_task(name="apikey_sync.sync_all_budgets")
def sync_all_apikey_budgets():
    """
    åŒæ­¥æ‰€æœ‰ API Key çš„é¢„ç®—ï¼ˆRedis â†’ DBï¼‰
    
    æ‰§è¡Œé¢‘ç‡ï¼šæ¯ 10 åˆ†é’Ÿ
    """
    logger.info("Starting API Key budget sync task")
    
    with get_sync_session() as session:
        # è·å–æ‰€æœ‰ API Key
        stmt = select(ApiKey.id).where(ApiKey.is_active == True)
        result = session.execute(stmt)
        api_key_ids = [str(row[0]) for row in result.fetchall()]
        
        success_count = 0
        failure_count = 0
        
        for api_key_id in api_key_ids:
            try:
                sync_apikey_budget(api_key_id, session)
                success_count += 1
            except Exception as e:
                logger.error(f"Failed to sync budget for api_key={api_key_id}: {e}")
                failure_count += 1
        
        logger.info(
            f"API Key budget sync completed: success={success_count}, failure={failure_count}"
        )
    
    return {
        "success_count": success_count,
        "failure_count": failure_count,
        "total": len(api_key_ids),
    }


def sync_apikey_budget(api_key_id: str, session: Session) -> None:
    """
    åŒæ­¥å•ä¸ª API Key çš„é¢„ç®—ï¼ˆRedis â†’ DBï¼‰
    """
    redis_client = getattr(cache, "_redis_sync", None)
    if not redis_client:
        logger.warning("Redis not available, skip sync")
        return
    
    # ä» Redis è¯»å– budget_used
    key = CacheKeys.apikey_budget_hash(api_key_id)
    budget_used = redis_client.hget(cache._make_key(key), b"budget_used")
    
    if budget_used is None:
        logger.debug(f"No Redis data for api_key={api_key_id}, skip")
        return
    
    budget_used = Decimal(budget_used.decode())
    
    # ä» DB è¯»å–é…é¢
    stmt = select(ApiKeyQuota).where(
        ApiKeyQuota.api_key_id == api_key_id,
        ApiKeyQuota.quota_type == QuotaType.BUDGET
    )
    result = session.execute(stmt)
    quota = result.scalars().first()
    
    if not quota:
        logger.warning(f"No budget quota found for api_key={api_key_id}")
        return
    
    # æ¯”è¾ƒå·®å¼‚
    db_budget_used = Decimal(quota.used_quota) / 100  # åˆ†è½¬å…ƒ
    diff = float(budget_used - db_budget_used)
    
    if abs(diff) > 0.01:
        logger.warning(
            f"Budget diff detected for api_key={api_key_id}: "
            f"redis={budget_used:.6f}, db={db_budget_used:.6f}, diff={diff:.6f}"
        )
        
        # æ›´æ–° DBï¼ˆä»¥ Redis ä¸ºå‡†ï¼‰
        quota.used_quota = int(budget_used * 100)  # å…ƒè½¬åˆ†
        session.commit()
        
        logger.info(f"Synced budget for api_key={api_key_id}")
```



---

## 4ï¸âƒ£ äº‹åŠ¡åè°ƒåº¦å™¨ï¼šTransactionAwareCelery

### è®¾è®¡æ€è·¯

**é—®é¢˜**ï¼š
- å½“å‰ Celery ä»»åŠ¡åœ¨äº‹åŠ¡æäº¤å‰å°±è§¦å‘
- å¦‚æœäº‹åŠ¡å›æ»šï¼Œä»»åŠ¡å·²ç»å‘é€ï¼Œå¯¼è‡´æ•°æ®ä¸ä¸€è‡´

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å°è£… TransactionAwareCelery
- ä»»åŠ¡åœ¨äº‹åŠ¡æäº¤åæ‰çœŸæ­£å‘é€
- ä½¿ç”¨ SQLAlchemy çš„ after_commit é’©å­

### å®ç°ä»£ç 

#### 4.1 åˆ›å»ºäº‹åŠ¡æ„ŸçŸ¥çš„ Celery å°è£…

```python
# backend/app/core/transaction_celery.py

"""
äº‹åŠ¡æ„ŸçŸ¥çš„ Celery ä»»åŠ¡è°ƒåº¦å™¨

èŒè´£ï¼š
- ç¡®ä¿ Celery ä»»åŠ¡åœ¨äº‹åŠ¡æäº¤åæ‰å‘é€
- é¿å…äº‹åŠ¡å›æ»šå¯¼è‡´çš„æ•°æ®ä¸ä¸€è‡´
"""

import logging
from typing import Any, Dict, Optional
from contextvars import ContextVar

from celery import Task
from sqlalchemy import event
from sqlalchemy.orm import Session

logger = logging.getLogger(__name__)

# ä½¿ç”¨ ContextVar å­˜å‚¨å¾…å‘é€çš„ä»»åŠ¡
_pending_tasks: ContextVar[list] = ContextVar("pending_tasks", default=[])


class TransactionAwareTask:
    """äº‹åŠ¡æ„ŸçŸ¥çš„ä»»åŠ¡åŒ…è£…å™¨"""
    
    def __init__(
        self,
        task: Task,
        args: tuple = (),
        kwargs: dict = None,
        options: dict = None,
    ):
        self.task = task
        self.args = args
        self.kwargs = kwargs or {}
        self.options = options or {}
    
    def send(self):
        """å‘é€ä»»åŠ¡åˆ° Celery"""
        try:
            self.task.apply_async(
                args=self.args,
                kwargs=self.kwargs,
                **self.options
            )
            logger.debug(f"Sent task: {self.task.name}")
        except Exception as e:
            logger.error(f"Failed to send task {self.task.name}: {e}")


def delay_after_commit(
    session: Session,
    task: Task,
    *args,
    **kwargs
) -> None:
    """
    åœ¨äº‹åŠ¡æäº¤åå»¶è¿Ÿå‘é€ä»»åŠ¡
    
    ç”¨æ³•ï¼š
        delay_after_commit(session, record_usage_task, usage_data)
    
    Args:
        session: SQLAlchemy Session
        task: Celery Task
        *args: ä»»åŠ¡å‚æ•°
        **kwargs: ä»»åŠ¡å…³é”®å­—å‚æ•°
    """
    # åˆ›å»ºä»»åŠ¡åŒ…è£…å™¨
    task_wrapper = TransactionAwareTask(task, args, kwargs)
    
    # è·å–å½“å‰å¾…å‘é€ä»»åŠ¡åˆ—è¡¨
    pending = _pending_tasks.get()
    pending.append(task_wrapper)
    _pending_tasks.set(pending)
    
    # æ³¨å†Œäº‹åŠ¡åé’©å­ï¼ˆåªæ³¨å†Œä¸€æ¬¡ï¼‰
    if not hasattr(session, "_celery_hook_registered"):
        @event.listens_for(session, "after_commit", once=True)
        def send_pending_tasks(session):
            """äº‹åŠ¡æäº¤åå‘é€æ‰€æœ‰å¾…å‘é€ä»»åŠ¡"""
            pending = _pending_tasks.get()
            
            logger.debug(f"Sending {len(pending)} pending tasks after commit")
            
            for task_wrapper in pending:
                task_wrapper.send()
            
            # æ¸…ç©ºå¾…å‘é€ä»»åŠ¡åˆ—è¡¨
            _pending_tasks.set([])
        
        @event.listens_for(session, "after_rollback", once=True)
        def clear_pending_tasks(session):
            """äº‹åŠ¡å›æ»šåæ¸…ç©ºå¾…å‘é€ä»»åŠ¡"""
            pending = _pending_tasks.get()
            
            logger.warning(f"Cleared {len(pending)} pending tasks after rollback")
            
            # æ¸…ç©ºå¾…å‘é€ä»»åŠ¡åˆ—è¡¨
            _pending_tasks.set([])
        
        session._celery_hook_registered = True


class TransactionAwareCelery:
    """
    äº‹åŠ¡æ„ŸçŸ¥çš„ Celery åŒ…è£…å™¨
    
    ç”¨æ³•ï¼š
        from app.core.transaction_celery import transaction_celery
        
        # åœ¨äº‹åŠ¡å†…
        transaction_celery.delay_after_commit(
            session,
            record_usage_task,
            usage_data
        )
    """
    
    @staticmethod
    def delay_after_commit(
        session: Session,
        task: Task,
        *args,
        **kwargs
    ) -> None:
        """åœ¨äº‹åŠ¡æäº¤åå»¶è¿Ÿå‘é€ä»»åŠ¡"""
        delay_after_commit(session, task, *args, **kwargs)


# å…¨å±€å®ä¾‹
transaction_celery = TransactionAwareCelery()
```

#### 4.2 ä¿®æ”¹ BillingStep ä½¿ç”¨äº‹åŠ¡æ„ŸçŸ¥è°ƒåº¦å™¨

```python
# backend/app/services/workflow/steps/billing.py

from app.core.transaction_celery import transaction_celery

async def _record_usage(self, ctx: "WorkflowContext") -> None:
    """
    è®°å½•ç”¨é‡ï¼ˆä½¿ç”¨äº‹åŠ¡æ„ŸçŸ¥è°ƒåº¦å™¨ï¼‰
    
    å˜æ›´ï¼š
    - ä½¿ç”¨ transaction_celery.delay_after_commit()
    - ç¡®ä¿ä»»åŠ¡åœ¨äº‹åŠ¡æäº¤åæ‰å‘é€
    """
    try:
        from app.tasks.billing import record_usage_task
        
        usage_data = {
            "tenant_id": str(ctx.tenant_id) if ctx.tenant_id else None,
            "api_key_id": str(ctx.api_key_id) if ctx.api_key_id else None,
            "trace_id": ctx.trace_id,
            "model": ctx.requested_model,
            "capability": ctx.capability,
            "input_tokens": ctx.billing.input_tokens,
            "output_tokens": ctx.billing.output_tokens,
            "total_cost": ctx.billing.total_cost,
            "currency": ctx.billing.currency,
            "provider": ctx.upstream_result.provider,
            "latency_ms": ctx.upstream_result.latency_ms,
            "is_error": not ctx.is_success,
        }
        
        # ä½¿ç”¨äº‹åŠ¡æ„ŸçŸ¥è°ƒåº¦å™¨
        transaction_celery.delay_after_commit(
            ctx.db_session.sync_session,  # è·å–åŒæ­¥ session
            record_usage_task,
            usage_data
        )
        
    except Exception as exc:
        logger.warning(f"Usage task dispatch failed: {exc}")
```

---

## 5ï¸âƒ£ ä¼šè¯åˆ†å¸ƒå¼é”

### è®¾è®¡æ€è·¯

**é—®é¢˜**ï¼š
- åŒä¸€ session_id çš„å¹¶å‘è¯·æ±‚å¯èƒ½å¯¼è‡´æ¶ˆæ¯é¡ºåºé”™ä¹±
- ä¼šè¯å†å²æ··ä¹±ã€æ‘˜è¦ä¸å‡†ç¡®

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨ Redis åˆ†å¸ƒå¼é”ä¿æŠ¤ä¼šè¯å†™å…¥
- åŒä¸€ session_id çš„è¯·æ±‚ä¸²è¡Œæ‰§è¡Œ

### å®ç°ä»£ç 

#### 5.1 åˆ›å»ºåˆ†å¸ƒå¼é”å·¥å…·

```python
# backend/app/core/distributed_lock.py

"""
åˆ†å¸ƒå¼é”å·¥å…·

èŒè´£ï¼š
- åŸºäº Redis å®ç°åˆ†å¸ƒå¼é”
- æ”¯æŒè‡ªåŠ¨ç»­æœŸ
- æ”¯æŒè¶…æ—¶é‡Šæ”¾
"""

import asyncio
import logging
import time
import uuid
from typing import Optional

from app.core.cache import cache

logger = logging.getLogger(__name__)


class DistributedLock:
    """
    åˆ†å¸ƒå¼é”
    
    ç”¨æ³•ï¼š
        async with DistributedLock("session:123", timeout=30):
            # ä¸´ç•ŒåŒºä»£ç 
            pass
    """
    
    def __init__(
        self,
        key: str,
        timeout: int = 30,
        retry_interval: float = 0.1,
        max_retries: int = 100,
    ):
        """
        åˆå§‹åŒ–åˆ†å¸ƒå¼é”
        
        Args:
            key: é”çš„é”®å
            timeout: é”çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            retry_interval: è·å–é”å¤±è´¥åçš„é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.key = f"lock:{key}"
        self.timeout = timeout
        self.retry_interval = retry_interval
        self.max_retries = max_retries
        self.lock_id = str(uuid.uuid4())
        self._acquired = False
    
    async def acquire(self) -> bool:
        """
        è·å–é”
        
        Returns:
            æ˜¯å¦æˆåŠŸè·å–é”
        """
        redis_client = getattr(cache, "_redis", None)
        if not redis_client:
            logger.warning("Redis not available, skip lock")
            return True  # Redis ä¸å¯ç”¨æ—¶ç›´æ¥æ”¾è¡Œ
        
        for attempt in range(self.max_retries):
            # å°è¯•è®¾ç½®é”ï¼ˆNX: ä¸å­˜åœ¨æ—¶æ‰è®¾ç½®ï¼ŒEX: è®¾ç½®è¿‡æœŸæ—¶é—´ï¼‰
            acquired = await redis_client.set(
                cache._make_key(self.key),
                self.lock_id,
                ex=self.timeout,
                nx=True
            )
            
            if acquired:
                self._acquired = True
                logger.debug(f"Acquired lock: {self.key}")
                return True
            
            # è·å–é”å¤±è´¥ï¼Œç­‰å¾…åé‡è¯•
            await asyncio.sleep(self.retry_interval)
        
        logger.warning(f"Failed to acquire lock after {self.max_retries} retries: {self.key}")
        return False
    
    async def release(self) -> None:
        """é‡Šæ”¾é”"""
        if not self._acquired:
            return
        
        redis_client = getattr(cache, "_redis", None)
        if not redis_client:
            return
        
        # ä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿åªé‡Šæ”¾è‡ªå·±æŒæœ‰çš„é”
        lua_script = """
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
        """
        
        try:
            await redis_client.eval(
                lua_script,
                1,
                cache._make_key(self.key),
                self.lock_id
            )
            logger.debug(f"Released lock: {self.key}")
        except Exception as e:
            logger.error(f"Failed to release lock {self.key}: {e}")
        finally:
            self._acquired = False
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        acquired = await self.acquire()
        if not acquired:
            raise TimeoutError(f"Failed to acquire lock: {self.key}")
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.release()
```

#### 5.2 ä¿®æ”¹ä¼šè¯æ­¥éª¤ä½¿ç”¨åˆ†å¸ƒå¼é”

```python
# backend/app/services/workflow/steps/conversation_append.py

from app.core.distributed_lock import DistributedLock

@step_registry.register
class ConversationAppendStep(BaseStep):
    """
    ä¼šè¯è¿½åŠ æ­¥éª¤ï¼ˆä½¿ç”¨åˆ†å¸ƒå¼é”ï¼‰
    
    å˜æ›´ï¼š
    - ä½¿ç”¨åˆ†å¸ƒå¼é”ä¿æŠ¤ä¼šè¯å†™å…¥
    - é˜²æ­¢å¹¶å‘å†™å…¥å¯¼è‡´çš„æ¶ˆæ¯é¡ºåºé”™ä¹±
    """
    
    name = "conversation_append"
    depends_on = ["response_transform"]
    
    async def execute(self, ctx: "WorkflowContext") -> StepResult:
        """æ‰§è¡Œä¼šè¯è¿½åŠ ï¼ˆä½¿ç”¨åˆ†å¸ƒå¼é”ï¼‰"""
        session_id = ctx.get("conversation_load", "session_id")
        if not session_id:
            return StepResult(status=StepStatus.SUCCESS)
        
        # ä½¿ç”¨åˆ†å¸ƒå¼é”ä¿æŠ¤ä¼šè¯å†™å…¥
        try:
            async with DistributedLock(f"session:{session_id}", timeout=30):
                return await self._append_messages(ctx, session_id)
        except TimeoutError as e:
            logger.error(f"Failed to acquire session lock: {e}")
            ctx.mark_error(
                ErrorSource.GATEWAY,
                "SESSION_LOCK_TIMEOUT",
                "Failed to acquire session lock",
            )
            return StepResult(status=StepStatus.FAILED, message=str(e))
    
    async def _append_messages(
        self,
        ctx: "WorkflowContext",
        session_id: str,
    ) -> StepResult:
        """
        è¿½åŠ æ¶ˆæ¯åˆ°ä¼šè¯
        
        æµç¨‹ï¼š
        1. è·å–ç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹å›å¤
        2. è¿½åŠ åˆ°ä¼šè¯å†å²
        3. æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘æ‘˜è¦
        4. ä¿å­˜ä¼šè¯
        """
        # ... åŸæœ‰é€»è¾‘ ...
        
        logger.info(
            f"Appended messages to session: session_id={session_id}, "
            f"message_count={len(messages)}"
        )
        
        return StepResult(status=StepStatus.SUCCESS)
```



---

## 6ï¸âƒ£ è·¯ç”±äº²å’ŒçŠ¶æ€æœº

### è®¾è®¡æ€è·¯

**é—®é¢˜**ï¼š
- å½“å‰è·¯ç”±äº²å’Œæ›´æ–°æ—¶æœºä¸æ˜ç¡®
- å¤±è´¥é‡è¯•åˆ‡æ¢ä¸Šæ¸¸åæŒ‡å‘ä¸ç¡®å®š

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å®ç°è·¯ç”±äº²å’ŒçŠ¶æ€æœº
- åªåœ¨æœ€ç»ˆæˆåŠŸæ—¶æ›´æ–°äº²å’Œ
- å¤±è´¥æ—¶æ¸…é™¤äº²å’Œ

### å®ç°ä»£ç 

#### 6.1 åˆ›å»ºè·¯ç”±äº²å’ŒçŠ¶æ€æœº

```python
# backend/app/services/routing/affinity.py

"""
è·¯ç”±äº²å’ŒçŠ¶æ€æœº

èŒè´£ï¼š
- ç®¡ç†è·¯ç”±äº²å’ŒçŠ¶æ€
- åªåœ¨æœ€ç»ˆæˆåŠŸæ—¶æ›´æ–°äº²å’Œ
- å¤±è´¥æ—¶æ¸…é™¤äº²å’Œ
"""

import logging
from enum import Enum
from typing import Optional

from app.core.cache import cache
from app.core.cache_keys import CacheKeys

logger = logging.getLogger(__name__)


class AffinityState(str, Enum):
    """äº²å’ŒçŠ¶æ€"""
    NONE = "none"  # æ— äº²å’Œ
    PENDING = "pending"  # å¾…ç¡®è®¤ï¼ˆè¯·æ±‚ä¸­ï¼‰
    ACTIVE = "active"  # æ´»è·ƒï¼ˆæˆåŠŸï¼‰
    FAILED = "failed"  # å¤±è´¥


class RoutingAffinity:
    """
    è·¯ç”±äº²å’Œç®¡ç†å™¨
    
    çŠ¶æ€è½¬æ¢ï¼š
    NONE -> PENDING -> ACTIVE (æˆåŠŸ)
    NONE -> PENDING -> FAILED (å¤±è´¥) -> NONE (æ¸…é™¤)
    """
    
    def __init__(self, session_id: str, model: str):
        self.session_id = session_id
        self.model = model
        self.key = CacheKeys.routing_affinity(session_id, model)
    
    async def get_affinity(self) -> Optional[str]:
        """
        è·å–å½“å‰äº²å’Œçš„ provider_model_id
        
        Returns:
            provider_model_id æˆ– None
        """
        redis_client = getattr(cache, "_redis", None)
        if not redis_client:
            return None
        
        try:
            data = await redis_client.hgetall(cache._make_key(self.key))
            if not data:
                return None
            
            state = data.get(b"state", b"").decode()
            provider_model_id = data.get(b"provider_model_id", b"").decode()
            
            # åªè¿”å› ACTIVE çŠ¶æ€çš„äº²å’Œ
            if state == AffinityState.ACTIVE and provider_model_id:
                return provider_model_id
            
            return None
        except Exception as e:
            logger.error(f"Failed to get affinity: {e}")
            return None
    
    async def set_pending(self, provider_model_id: str) -> None:
        """
        è®¾ç½®äº²å’Œä¸º PENDING çŠ¶æ€ï¼ˆè¯·æ±‚å¼€å§‹æ—¶ï¼‰
        
        Args:
            provider_model_id: é€‰æ‹©çš„ provider_model_id
        """
        redis_client = getattr(cache, "_redis", None)
        if not redis_client:
            return
        
        try:
            await redis_client.hset(
                cache._make_key(self.key),
                mapping={
                    "state": AffinityState.PENDING,
                    "provider_model_id": provider_model_id,
                    "updated_at": str(int(time.time())),
                }
            )
            await redis_client.expire(cache._make_key(self.key), 3600)  # 1 å°æ—¶è¿‡æœŸ
            
            logger.debug(
                f"Set affinity to PENDING: session_id={self.session_id}, "
                f"provider_model_id={provider_model_id}"
            )
        except Exception as e:
            logger.error(f"Failed to set pending affinity: {e}")
    
    async def confirm_success(self, provider_model_id: str) -> None:
        """
        ç¡®è®¤äº²å’ŒæˆåŠŸï¼ˆè¯·æ±‚æˆåŠŸæ—¶ï¼‰
        
        Args:
            provider_model_id: æˆåŠŸçš„ provider_model_id
        """
        redis_client = getattr(cache, "_redis", None)
        if not redis_client:
            return
        
        try:
            await redis_client.hset(
                cache._make_key(self.key),
                mapping={
                    "state": AffinityState.ACTIVE,
                    "provider_model_id": provider_model_id,
                    "updated_at": str(int(time.time())),
                    "success_count": await self._increment_success_count(),
                }
            )
            await redis_client.expire(cache._make_key(self.key), 3600)  # 1 å°æ—¶è¿‡æœŸ
            
            logger.info(
                f"Confirmed affinity success: session_id={self.session_id}, "
                f"provider_model_id={provider_model_id}"
            )
        except Exception as e:
            logger.error(f"Failed to confirm affinity: {e}")
    
    async def mark_failed(self) -> None:
        """
        æ ‡è®°äº²å’Œå¤±è´¥ï¼ˆè¯·æ±‚å¤±è´¥æ—¶ï¼‰
        """
        redis_client = getattr(cache, "_redis", None)
        if not redis_client:
            return
        
        try:
            await redis_client.hset(
                cache._make_key(self.key),
                mapping={
                    "state": AffinityState.FAILED,
                    "updated_at": str(int(time.time())),
                }
            )
            
            logger.warning(f"Marked affinity as failed: session_id={self.session_id}")
        except Exception as e:
            logger.error(f"Failed to mark affinity as failed: {e}")
    
    async def clear(self) -> None:
        """
        æ¸…é™¤äº²å’Œï¼ˆå¤±è´¥åæ¸…é™¤ï¼‰
        """
        redis_client = getattr(cache, "_redis", None)
        if not redis_client:
            return
        
        try:
            await redis_client.delete(cache._make_key(self.key))
            
            logger.info(f"Cleared affinity: session_id={self.session_id}")
        except Exception as e:
            logger.error(f"Failed to clear affinity: {e}")
    
    async def _increment_success_count(self) -> int:
        """å¢åŠ æˆåŠŸè®¡æ•°"""
        redis_client = getattr(cache, "_redis", None)
        if not redis_client:
            return 0
        
        try:
            count = await redis_client.hincrby(
                cache._make_key(self.key),
                "success_count",
                1
            )
            return count
        except Exception:
            return 0
```

#### 6.2 ä¿®æ”¹è·¯ç”±æ­¥éª¤ä½¿ç”¨äº²å’ŒçŠ¶æ€æœº

```python
# backend/app/services/workflow/steps/routing.py

from app.services.routing.affinity import RoutingAffinity

@step_registry.register
class RoutingStep(BaseStep):
    """
    è·¯ç”±æ­¥éª¤ï¼ˆä½¿ç”¨äº²å’ŒçŠ¶æ€æœºï¼‰
    
    å˜æ›´ï¼š
    - ä½¿ç”¨ RoutingAffinity ç®¡ç†äº²å’ŒçŠ¶æ€
    - è¯·æ±‚å¼€å§‹æ—¶è®¾ç½® PENDING
    - è¯·æ±‚æˆåŠŸæ—¶ç¡®è®¤ ACTIVE
    - è¯·æ±‚å¤±è´¥æ—¶æ¸…é™¤äº²å’Œ
    """
    
    name = "routing"
    depends_on = ["validation"]
    
    async def execute(self, ctx: "WorkflowContext") -> StepResult:
        """æ‰§è¡Œè·¯ç”±å†³ç­–"""
        # è·å–ä¼šè¯ ID
        session_id = ctx.get("conversation_load", "session_id")
        model = ctx.requested_model
        
        # åˆ›å»ºäº²å’Œç®¡ç†å™¨
        affinity = RoutingAffinity(session_id, model) if session_id else None
        
        # è·å–äº²å’Œçš„ provider_model_id
        affinity_provider_model_id = None
        if affinity:
            affinity_provider_model_id = await affinity.get_affinity()
        
        # è·¯ç”±å†³ç­–
        selected = await self._select_provider_model(
            ctx,
            model,
            affinity_provider_model_id
        )
        
        if not selected:
            ctx.mark_error(
                ErrorSource.GATEWAY,
                "ROUTING_NO_PROVIDER",
                f"No available provider for model: {model}",
            )
            return StepResult(status=StepStatus.FAILED)
        
        # è®¾ç½®äº²å’Œä¸º PENDING
        if affinity:
            await affinity.set_pending(str(selected.id))
        
        # å†™å…¥ä¸Šä¸‹æ–‡
        ctx.set("routing", "provider_model_id", str(selected.id))
        ctx.set("routing", "provider", selected.provider)
        ctx.set("routing", "upstream_model", selected.upstream_model)
        ctx.set("routing", "affinity_manager", affinity)  # ä¿å­˜äº²å’Œç®¡ç†å™¨
        
        return StepResult(status=StepStatus.SUCCESS)
```

#### 6.3 ä¿®æ”¹ä¸Šæ¸¸è°ƒç”¨æ­¥éª¤ç¡®è®¤äº²å’Œ

```python
# backend/app/services/workflow/steps/upstream_call.py

@step_registry.register
class UpstreamCallStep(BaseStep):
    """
    ä¸Šæ¸¸è°ƒç”¨æ­¥éª¤ï¼ˆç¡®è®¤äº²å’Œï¼‰
    
    å˜æ›´ï¼š
    - è¯·æ±‚æˆåŠŸæ—¶ç¡®è®¤äº²å’Œ
    - è¯·æ±‚å¤±è´¥æ—¶æ¸…é™¤äº²å’Œ
    """
    
    name = "upstream_call"
    depends_on = ["template_render"]
    
    async def execute(self, ctx: "WorkflowContext") -> StepResult:
        """æ‰§è¡Œä¸Šæ¸¸è°ƒç”¨"""
        try:
            # è°ƒç”¨ä¸Šæ¸¸
            result = await self._call_upstream(ctx)
            
            # è¯·æ±‚æˆåŠŸï¼Œç¡®è®¤äº²å’Œ
            affinity = ctx.get("routing", "affinity_manager")
            if affinity:
                provider_model_id = ctx.get("routing", "provider_model_id")
                await affinity.confirm_success(provider_model_id)
            
            return StepResult(status=StepStatus.SUCCESS, data=result)
            
        except Exception as e:
            logger.error(f"Upstream call failed: {e}")
            
            # è¯·æ±‚å¤±è´¥ï¼Œæ¸…é™¤äº²å’Œ
            affinity = ctx.get("routing", "affinity_manager")
            if affinity:
                await affinity.mark_failed()
                await affinity.clear()
            
            ctx.mark_error(
                ErrorSource.UPSTREAM,
                "UPSTREAM_CALL_FAILED",
                str(e),
            )
            return StepResult(status=StepStatus.FAILED, message=str(e))
```

---

## 7ï¸âƒ£ ç±»å‹åŒ– Context

### è®¾è®¡æ€è·¯

**é—®é¢˜**ï¼š
- å½“å‰ Context ä½¿ç”¨å­—å…¸å­˜å‚¨ï¼Œç±»å‹ä¸å®‰å…¨
- å®¹æ˜“å‡ºç°é”®åå†²çªå’Œæ‹¼å†™é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
- åˆ›å»ºç±»å‹åŒ–çš„ Context å­—æ®µ
- ä½¿ç”¨ Pydantic æ¨¡å‹å®šä¹‰å­—æ®µ
- æä¾›ç±»å‹æç¤ºå’Œè‡ªåŠ¨è¡¥å…¨

### å®ç°ä»£ç 

#### 7.1 åˆ›å»ºç±»å‹åŒ– Context å­—æ®µ

```python
# backend/app/services/orchestrator/typed_context.py

"""
ç±»å‹åŒ– Context å­—æ®µ

èŒè´£ï¼š
- å®šä¹‰ Context ä¸­çš„å­—æ®µç±»å‹
- æä¾›ç±»å‹æç¤ºå’Œè‡ªåŠ¨è¡¥å…¨
- é˜²æ­¢é”®åå†²çªå’Œæ‹¼å†™é”™è¯¯
"""

from typing import Optional
from decimal import Decimal
from pydantic import BaseModel, Field


class ValidationContext(BaseModel):
    """validation æ­¥éª¤çš„ Context"""
    request: Optional[object] = None
    validated: bool = False


class QuotaCheckContext(BaseModel):
    """quota_check æ­¥éª¤çš„ Context"""
    balance_before: Decimal = Decimal("0")
    balance_after: Decimal = Decimal("0")
    daily_used: int = 0
    monthly_used: int = 0
    estimated_cost: float = 0.0
    remaining_balance: float = 0.0
    daily_remaining: int = 0
    monthly_remaining: int = 0


class RoutingContext(BaseModel):
    """routing æ­¥éª¤çš„ Context"""
    provider_model_id: Optional[str] = None
    provider: Optional[str] = None
    upstream_model: Optional[str] = None
    pricing_config: Optional[dict] = None
    affinity_manager: Optional[object] = None


class BillingContext(BaseModel):
    """billing æ­¥éª¤çš„ Context"""
    pending_transaction_id: Optional[str] = None
    pending_trace_id: Optional[str] = None
    pricing_config: Optional[dict] = None
    balance_after: float = 0.0
    skip_reason: Optional[str] = None


class UpstreamCallContext(BaseModel):
    """upstream_call æ­¥éª¤çš„ Context"""
    stream: bool = False
    response_stream: Optional[object] = None
    stream_accumulator: Optional[object] = None
    status_code: int = 200


class TypedContextFields:
    """
    ç±»å‹åŒ– Context å­—æ®µé›†åˆ
    
    ç”¨æ³•ï¼š
        # å†™å…¥
        ctx.typed.quota_check.balance_before = Decimal("100.00")
        
        # è¯»å–
        balance = ctx.typed.quota_check.balance_before
    """
    
    def __init__(self, context: "WorkflowContext"):
        self._context = context
        self._validation: Optional[ValidationContext] = None
        self._quota_check: Optional[QuotaCheckContext] = None
        self._routing: Optional[RoutingContext] = None
        self._billing: Optional[BillingContext] = None
        self._upstream_call: Optional[UpstreamCallContext] = None
    
    @property
    def validation(self) -> ValidationContext:
        """è·å– validation Context"""
        if self._validation is None:
            self._validation = ValidationContext()
        return self._validation
    
    @property
    def quota_check(self) -> QuotaCheckContext:
        """è·å– quota_check Context"""
        if self._quota_check is None:
            self._quota_check = QuotaCheckContext()
        return self._quota_check
    
    @property
    def routing(self) -> RoutingContext:
        """è·å– routing Context"""
        if self._routing is None:
            self._routing = RoutingContext()
        return self._routing
    
    @property
    def billing(self) -> BillingContext:
        """è·å– billing Context"""
        if self._billing is None:
            self._billing = BillingContext()
        return self._billing
    
    @property
    def upstream_call(self) -> UpstreamCallContext:
        """è·å– upstream_call Context"""
        if self._upstream_call is None:
            self._upstream_call = UpstreamCallContext()
        return self._upstream_call
```

#### 7.2 ä¿®æ”¹ WorkflowContext æ”¯æŒç±»å‹åŒ–å­—æ®µ

```python
# backend/app/services/orchestrator/context.py

from app.services.orchestrator.typed_context import TypedContextFields

class WorkflowContext:
    """
    å·¥ä½œæµä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒç±»å‹åŒ–å­—æ®µï¼‰
    
    å˜æ›´ï¼š
    - æ–°å¢ typed å±æ€§ï¼Œæä¾›ç±»å‹åŒ–å­—æ®µè®¿é—®
    - ä¿ç•™åŸæœ‰çš„ get/set æ–¹æ³•ï¼Œå‘åå…¼å®¹
    """
    
    def __init__(self, ...):
        # ... åŸæœ‰åˆå§‹åŒ– ...
        
        # æ–°å¢ï¼šç±»å‹åŒ–å­—æ®µ
        self.typed = TypedContextFields(self)
    
    # ä¿ç•™åŸæœ‰çš„ get/set æ–¹æ³•
    def get(self, namespace: str, key: str, default=None):
        """è·å–ä¸Šä¸‹æ–‡å€¼ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self._data.get(namespace, {}).get(key, default)
    
    def set(self, namespace: str, key: str, value):
        """è®¾ç½®ä¸Šä¸‹æ–‡å€¼ï¼ˆå‘åå…¼å®¹ï¼‰"""
        if namespace not in self._data:
            self._data[namespace] = {}
        self._data[namespace][key] = value
```

#### 7.3 ä½¿ç”¨ç¤ºä¾‹

```python
# ä½¿ç”¨ç±»å‹åŒ–å­—æ®µï¼ˆæ¨èï¼‰
ctx.typed.quota_check.balance_before = Decimal("100.00")
ctx.typed.quota_check.balance_after = Decimal("99.50")
balance = ctx.typed.quota_check.balance_after  # ç±»å‹æç¤ºï¼šDecimal

# ä½¿ç”¨åŸæœ‰æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰
ctx.set("quota_check", "balance_before", Decimal("100.00"))
balance = ctx.get("quota_check", "balance_before")  # ç±»å‹æç¤ºï¼šAny
```



---

## 8ï¸âƒ£ è¡¥å……æµ‹è¯•

### 8.1 QuotaCheckStep æµ‹è¯•ï¼ˆæ‰£å‡é…é¢ï¼‰

```python
# backend/tests/test_quota_check_deduct.py

import pytest
from decimal import Decimal
from app.services.workflow.steps.quota_check import QuotaCheckStep, QuotaExceededError
from app.services.orchestrator.context import WorkflowContext, Channel

@pytest.mark.asyncio
async def test_quota_check_deduct_success(db_session, redis_client):
    """æµ‹è¯•é…é¢æ£€æŸ¥å¹¶æ‰£å‡æˆåŠŸ"""
    # å‡†å¤‡æ•°æ®
    tenant_id = "test-tenant-123"
    quota = await create_test_quota(
        db_session,
        tenant_id=tenant_id,
        balance=Decimal("100.00"),
        daily_quota=1000,
        daily_used=0,
        monthly_quota=30000,
        monthly_used=0,
    )
    
    # é¢„çƒ­ Redis ç¼“å­˜
    await warm_redis_quota(redis_client, quota)
    
    # åˆ›å»ºä¸Šä¸‹æ–‡
    ctx = WorkflowContext(
        channel=Channel.EXTERNAL,
        capability="chat",
        requested_model="gpt-3.5-turbo",
        db_session=db_session,
        tenant_id=tenant_id,
    )
    
    # è®¾ç½®å®šä»·ï¼ˆç”¨äºä¼°ç®—è´¹ç”¨ï¼‰
    ctx.set("routing", "pricing_config", {
        "input_per_1k": 0.001,
        "output_per_1k": 0.002,
    })
    
    # æ‰§è¡Œæ£€æŸ¥å¹¶æ‰£å‡
    step = QuotaCheckStep()
    result = await step.execute(ctx)
    
    # éªŒè¯ç»“æœ
    assert result.status == StepStatus.SUCCESS
    
    # éªŒè¯ Context
    balance_before = ctx.typed.quota_check.balance_before
    balance_after = ctx.typed.quota_check.balance_after
    assert balance_before == Decimal("100.00")
    assert balance_after < balance_before  # å·²æ‰£å‡
    
    # éªŒè¯ Redis
    redis_balance = await redis_client.hget(
        f"gw:quota:tenant:{tenant_id}",
        "balance"
    )
    assert float(redis_balance) == float(balance_after)


@pytest.mark.asyncio
async def test_quota_check_concurrent_deduct(db_session, redis_client):
    """æµ‹è¯•å¹¶å‘æ‰£å‡çš„åŸå­æ€§"""
    # å‡†å¤‡æ•°æ®
    tenant_id = "test-tenant-123"
    quota = await create_test_quota(
        db_session,
        tenant_id=tenant_id,
        balance=Decimal("10.00"),
    )
    
    await warm_redis_quota(redis_client, quota)
    
    # åˆ›å»º 10 ä¸ªå¹¶å‘è¯·æ±‚
    tasks = []
    for i in range(10):
        ctx = WorkflowContext(
            channel=Channel.EXTERNAL,
            capability="chat",
            requested_model="gpt-3.5-turbo",
            db_session=db_session,
            tenant_id=tenant_id,
            trace_id=f"test-trace-{i}",
        )
        ctx.set("routing", "pricing_config", {
            "input_per_1k": 0.001,
            "output_per_1k": 0.002,
        })
        
        step = QuotaCheckStep()
        tasks.append(step.execute(ctx))
    
    # å¹¶å‘æ‰§è¡Œ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # éªŒè¯ç»“æœ
    success_count = sum(1 for r in results if isinstance(r, StepResult) and r.status == StepStatus.SUCCESS)
    
    # éªŒè¯ Redis ä½™é¢
    redis_balance = await redis_client.hget(
        f"gw:quota:tenant:{tenant_id}",
        "balance"
    )
    
    # ä½™é¢åº”è¯¥æ­£ç¡®æ‰£å‡ï¼ˆåŸå­æ€§ï¼‰
    expected_balance = 10.00 - (success_count * 0.01)  # å‡è®¾æ¯æ¬¡æ‰£å‡ 0.01
    assert abs(float(redis_balance) - expected_balance) < 0.001
```

### 8.2 äº‹åŠ¡æ„ŸçŸ¥è°ƒåº¦å™¨æµ‹è¯•

```python
# backend/tests/test_transaction_celery.py

import pytest
from unittest.mock import Mock, patch
from sqlalchemy.orm import Session

from app.core.transaction_celery import transaction_celery, delay_after_commit

@pytest.mark.asyncio
async def test_delay_after_commit_success(db_session):
    """æµ‹è¯•äº‹åŠ¡æäº¤åå‘é€ä»»åŠ¡"""
    # åˆ›å»º Mock ä»»åŠ¡
    mock_task = Mock()
    mock_task.name = "test_task"
    mock_task.apply_async = Mock()
    
    # å¼€å§‹äº‹åŠ¡
    async with db_session.begin():
        # å»¶è¿Ÿå‘é€ä»»åŠ¡
        transaction_celery.delay_after_commit(
            db_session.sync_session,
            mock_task,
            "arg1",
            kwarg1="value1"
        )
        
        # æ­¤æ—¶ä»»åŠ¡è¿˜æœªå‘é€
        assert mock_task.apply_async.call_count == 0
    
    # äº‹åŠ¡æäº¤åï¼Œä»»åŠ¡åº”è¯¥è¢«å‘é€
    assert mock_task.apply_async.call_count == 1
    mock_task.apply_async.assert_called_with(
        args=("arg1",),
        kwargs={"kwarg1": "value1"}
    )


@pytest.mark.asyncio
async def test_delay_after_rollback(db_session):
    """æµ‹è¯•äº‹åŠ¡å›æ»šåä¸å‘é€ä»»åŠ¡"""
    # åˆ›å»º Mock ä»»åŠ¡
    mock_task = Mock()
    mock_task.name = "test_task"
    mock_task.apply_async = Mock()
    
    try:
        async with db_session.begin():
            # å»¶è¿Ÿå‘é€ä»»åŠ¡
            transaction_celery.delay_after_commit(
                db_session.sync_session,
                mock_task,
                "arg1"
            )
            
            # æŠ›å‡ºå¼‚å¸¸è§¦å‘å›æ»š
            raise Exception("Test rollback")
    except Exception:
        pass
    
    # äº‹åŠ¡å›æ»šåï¼Œä»»åŠ¡ä¸åº”è¯¥è¢«å‘é€
    assert mock_task.apply_async.call_count == 0
```

### 8.3 åˆ†å¸ƒå¼é”æµ‹è¯•

```python
# backend/tests/test_distributed_lock.py

import pytest
import asyncio
from app.core.distributed_lock import DistributedLock

@pytest.mark.asyncio
async def test_distributed_lock_acquire_release(redis_client):
    """æµ‹è¯•åˆ†å¸ƒå¼é”çš„è·å–å’Œé‡Šæ”¾"""
    lock = DistributedLock("test-resource", timeout=10)
    
    # è·å–é”
    acquired = await lock.acquire()
    assert acquired is True
    
    # éªŒè¯ Redis ä¸­å­˜åœ¨é”
    exists = await redis_client.exists("lock:test-resource")
    assert exists == 1
    
    # é‡Šæ”¾é”
    await lock.release()
    
    # éªŒè¯ Redis ä¸­é”å·²åˆ é™¤
    exists = await redis_client.exists("lock:test-resource")
    assert exists == 0


@pytest.mark.asyncio
async def test_distributed_lock_concurrent(redis_client):
    """æµ‹è¯•åˆ†å¸ƒå¼é”çš„å¹¶å‘æ§åˆ¶"""
    counter = {"value": 0}
    
    async def increment_with_lock():
        async with DistributedLock("test-counter", timeout=5):
            # ä¸´ç•ŒåŒºï¼šè¯»å–ã€å¢åŠ ã€å†™å…¥
            current = counter["value"]
            await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
            counter["value"] = current + 1
    
    # åˆ›å»º 10 ä¸ªå¹¶å‘ä»»åŠ¡
    tasks = [increment_with_lock() for _ in range(10)]
    await asyncio.gather(*tasks)
    
    # éªŒè¯è®¡æ•°å™¨å€¼æ­£ç¡®ï¼ˆæ— ç«æ€æ¡ä»¶ï¼‰
    assert counter["value"] == 10


@pytest.mark.asyncio
async def test_distributed_lock_timeout(redis_client):
    """æµ‹è¯•åˆ†å¸ƒå¼é”è¶…æ—¶"""
    # ç¬¬ä¸€ä¸ªé”æŒæœ‰èµ„æº
    lock1 = DistributedLock("test-resource", timeout=10)
    await lock1.acquire()
    
    # ç¬¬äºŒä¸ªé”å°è¯•è·å–ï¼ˆåº”è¯¥è¶…æ—¶ï¼‰
    lock2 = DistributedLock("test-resource", timeout=1, max_retries=5)
    
    with pytest.raises(TimeoutError):
        async with lock2:
            pass
```

### 8.4 è·¯ç”±äº²å’ŒçŠ¶æ€æœºæµ‹è¯•

```python
# backend/tests/test_routing_affinity.py

import pytest
from app.services.routing.affinity import RoutingAffinity, AffinityState

@pytest.mark.asyncio
async def test_affinity_state_machine(redis_client):
    """æµ‹è¯•è·¯ç”±äº²å’ŒçŠ¶æ€æœº"""
    affinity = RoutingAffinity("session-123", "gpt-3.5-turbo")
    
    # åˆå§‹çŠ¶æ€ï¼šæ— äº²å’Œ
    provider_model_id = await affinity.get_affinity()
    assert provider_model_id is None
    
    # è®¾ç½®ä¸º PENDING
    await affinity.set_pending("provider-model-1")
    
    # PENDING çŠ¶æ€ä¸è¿”å›äº²å’Œ
    provider_model_id = await affinity.get_affinity()
    assert provider_model_id is None
    
    # ç¡®è®¤æˆåŠŸï¼Œè®¾ç½®ä¸º ACTIVE
    await affinity.confirm_success("provider-model-1")
    
    # ACTIVE çŠ¶æ€è¿”å›äº²å’Œ
    provider_model_id = await affinity.get_affinity()
    assert provider_model_id == "provider-model-1"
    
    # æ ‡è®°å¤±è´¥
    await affinity.mark_failed()
    
    # æ¸…é™¤äº²å’Œ
    await affinity.clear()
    
    # æ¸…é™¤åæ— äº²å’Œ
    provider_model_id = await affinity.get_affinity()
    assert provider_model_id is None


@pytest.mark.asyncio
async def test_affinity_success_count(redis_client):
    """æµ‹è¯•äº²å’ŒæˆåŠŸè®¡æ•°"""
    affinity = RoutingAffinity("session-123", "gpt-3.5-turbo")
    
    # ç¬¬ä¸€æ¬¡æˆåŠŸ
    await affinity.set_pending("provider-model-1")
    await affinity.confirm_success("provider-model-1")
    
    # ç¬¬äºŒæ¬¡æˆåŠŸ
    await affinity.set_pending("provider-model-1")
    await affinity.confirm_success("provider-model-1")
    
    # ç¬¬ä¸‰æ¬¡æˆåŠŸ
    await affinity.set_pending("provider-model-1")
    await affinity.confirm_success("provider-model-1")
    
    # éªŒè¯æˆåŠŸè®¡æ•°
    data = await redis_client.hgetall(f"gw:routing:affinity:session-123:gpt-3.5-turbo")
    success_count = int(data.get(b"success_count", b"0"))
    assert success_count == 3
```

### 8.5 ç±»å‹åŒ– Context æµ‹è¯•

```python
# backend/tests/test_typed_context.py

import pytest
from decimal import Decimal
from app.services.orchestrator.context import WorkflowContext, Channel

def test_typed_context_quota_check():
    """æµ‹è¯•ç±»å‹åŒ– Context - quota_check"""
    ctx = WorkflowContext(
        channel=Channel.EXTERNAL,
        capability="chat",
        requested_model="gpt-3.5-turbo",
    )
    
    # ä½¿ç”¨ç±»å‹åŒ–å­—æ®µ
    ctx.typed.quota_check.balance_before = Decimal("100.00")
    ctx.typed.quota_check.balance_after = Decimal("99.50")
    ctx.typed.quota_check.daily_used = 1
    ctx.typed.quota_check.monthly_used = 1
    
    # éªŒè¯ç±»å‹
    assert isinstance(ctx.typed.quota_check.balance_before, Decimal)
    assert isinstance(ctx.typed.quota_check.balance_after, Decimal)
    assert isinstance(ctx.typed.quota_check.daily_used, int)
    assert isinstance(ctx.typed.quota_check.monthly_used, int)
    
    # éªŒè¯å€¼
    assert ctx.typed.quota_check.balance_before == Decimal("100.00")
    assert ctx.typed.quota_check.balance_after == Decimal("99.50")


def test_typed_context_backward_compatible():
    """æµ‹è¯•ç±»å‹åŒ– Context å‘åå…¼å®¹"""
    ctx = WorkflowContext(
        channel=Channel.EXTERNAL,
        capability="chat",
        requested_model="gpt-3.5-turbo",
    )
    
    # ä½¿ç”¨åŸæœ‰æ–¹æ³•
    ctx.set("quota_check", "balance_before", Decimal("100.00"))
    
    # ä½¿ç”¨ç±»å‹åŒ–å­—æ®µè¯»å–
    balance = ctx.typed.quota_check.balance_before
    
    # åº”è¯¥èƒ½è¯»å–åˆ°ï¼ˆä½†ç±»å‹å¯èƒ½ä¸åŒï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å®ç°åŒå‘åŒæ­¥
```

---

## ğŸ“Š å®æ–½è®¡åˆ’

### ç¬¬ 1 å‘¨ï¼šæ ¸å¿ƒåŠŸèƒ½

**Day 1-2**: è°ƒæ•´æµç¨‹ï¼ˆquota_check æ‰£å‡ï¼Œbilling å†™æµæ°´ï¼‰
- ä¿®æ”¹ QuotaCheckStep
- ä¿®æ”¹ BillingStep
- ä¿®æ”¹ BillingRepository
- æµ‹è¯•ï¼šå¹¶å‘æ‰£å‡ã€å¹‚ç­‰æ€§

**Day 3-4**: å‘¨æœŸæ€§åŒæ­¥ä»»åŠ¡
- åˆ›å»º quota_sync.py
- åˆ›å»º apikey_sync.py
- é…ç½® Celery Beat
- æµ‹è¯•ï¼šåŒæ­¥å‡†ç¡®æ€§ã€ä¸€è‡´æ€§æ£€æŸ¥

**Day 5**: äº‹åŠ¡åè°ƒåº¦å™¨
- åˆ›å»º transaction_celery.py
- ä¿®æ”¹ BillingStep
- æµ‹è¯•ï¼šäº‹åŠ¡æäº¤/å›æ»š

### ç¬¬ 2 å‘¨ï¼šé«˜çº§åŠŸèƒ½

**Day 6-7**: ä¼šè¯åˆ†å¸ƒå¼é”
- åˆ›å»º distributed_lock.py
- ä¿®æ”¹ ConversationAppendStep
- æµ‹è¯•ï¼šå¹¶å‘æ§åˆ¶ã€è¶…æ—¶

**Day 8-9**: è·¯ç”±äº²å’ŒçŠ¶æ€æœº
- åˆ›å»º affinity.py
- ä¿®æ”¹ RoutingStep
- ä¿®æ”¹ UpstreamCallStep
- æµ‹è¯•ï¼šçŠ¶æ€è½¬æ¢ã€æˆåŠŸè®¡æ•°

**Day 10**: ç±»å‹åŒ– Context
- åˆ›å»º typed_context.py
- ä¿®æ”¹ WorkflowContext
- æµ‹è¯•ï¼šç±»å‹å®‰å…¨ã€å‘åå…¼å®¹

---

## âœ… éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶

1. **é…é¢æ‰£å‡**
   - [ ] quota_check æ­¥éª¤åŸå­æ‰£å‡é…é¢
   - [ ] billing æ­¥éª¤åªå†™æµæ°´
   - [ ] å¹¶å‘è¯·æ±‚æ— é‡å¤æ‰£å‡
   - [ ] å®é™…è´¹ç”¨ä¸é¢„ä¼°ä¸åŒæ—¶æ­£ç¡®è°ƒæ•´

2. **å‘¨æœŸæ€§åŒæ­¥**
   - [ ] æ¯ 5 åˆ†é’ŸåŒæ­¥é…é¢åˆ° DB
   - [ ] æ¯ 10 åˆ†é’ŸåŒæ­¥ API Key é¢„ç®—åˆ° DB
   - [ ] æ¯ 1 å°æ—¶æ£€æŸ¥ä¸€è‡´æ€§
   - [ ] å·®å¼‚è¶…è¿‡é˜ˆå€¼æ—¶å‘é€å‘Šè­¦

3. **äº‹åŠ¡æ„ŸçŸ¥**
   - [ ] ä»»åŠ¡åœ¨äº‹åŠ¡æäº¤åæ‰å‘é€
   - [ ] äº‹åŠ¡å›æ»šæ—¶ä»»åŠ¡ä¸å‘é€
   - [ ] æ”¯æŒå¤šä¸ªä»»åŠ¡æ‰¹é‡å‘é€

4. **åˆ†å¸ƒå¼é”**
   - [ ] åŒä¸€ session_id çš„è¯·æ±‚ä¸²è¡Œæ‰§è¡Œ
   - [ ] é”è¶…æ—¶è‡ªåŠ¨é‡Šæ”¾
   - [ ] é”é‡Šæ”¾åå…¶ä»–è¯·æ±‚å¯è·å–

5. **è·¯ç”±äº²å’Œ**
   - [ ] åªåœ¨æœ€ç»ˆæˆåŠŸæ—¶æ›´æ–°äº²å’Œ
   - [ ] å¤±è´¥æ—¶æ¸…é™¤äº²å’Œ
   - [ ] æˆåŠŸè®¡æ•°æ­£ç¡®ç´¯åŠ 

6. **ç±»å‹åŒ– Context**
   - [ ] æä¾›ç±»å‹æç¤ºå’Œè‡ªåŠ¨è¡¥å…¨
   - [ ] å‘åå…¼å®¹åŸæœ‰ get/set æ–¹æ³•
   - [ ] é˜²æ­¢é”®åå†²çª

### æ€§èƒ½éªŒæ”¶

- [ ] quota_check P99 å»¶è¿Ÿ < 50ms
- [ ] billing P99 å»¶è¿Ÿ < 100ms
- [ ] åˆ†å¸ƒå¼é”è·å–å»¶è¿Ÿ < 10ms
- [ ] åŒæ­¥ä»»åŠ¡æ‰§è¡Œæ—¶é—´ < 5 åˆ†é’Ÿ

### ç›‘æ§éªŒæ”¶

- [ ] æ‰€æœ‰å…³é”®æ“ä½œæœ‰æ—¥å¿—
- [ ] æ‰€æœ‰å…³é”®æŒ‡æ ‡æœ‰ç›‘æ§
- [ ] å¼‚å¸¸æƒ…å†µæœ‰å‘Šè­¦
- [ ] Grafana Dashboard å®Œæ•´

---

## ğŸ‰ æ€»ç»“

æœ¬æ–‡æ¡£å®Œæ•´å®ç°äº†å‰©ä½™çš„ 8 ä¸ªåŠŸèƒ½ï¼š

1. âœ… è°ƒæ•´æµç¨‹ï¼šquota_check æ‰£å‡ï¼Œbilling å†™æµæ°´
2. âœ… å‘¨æœŸæ€§åŒæ­¥ä»»åŠ¡ï¼šRedis â†’ DB
3. âœ… API Key é¢„ç®—ï¼šRedis Hash + å®šæ—¶åŒæ­¥
4. âœ… äº‹åŠ¡åè°ƒåº¦å™¨ï¼šTransactionAwareCelery
5. âœ… ä¼šè¯åˆ†å¸ƒå¼é”
6. âœ… è·¯ç”±äº²å’ŒçŠ¶æ€æœº
7. âœ… ç±»å‹åŒ– Context
8. âœ… è¡¥å……æµ‹è¯•

æ‰€æœ‰åŠŸèƒ½éƒ½æä¾›äº†ï¼š
- å®Œæ•´çš„ä»£ç å®ç°
- è¯¦ç»†çš„æµ‹è¯•ç”¨ä¾‹
- æ¸…æ™°çš„å®æ–½è®¡åˆ’
- æ˜ç¡®çš„éªŒæ”¶æ ‡å‡†

é¢„è®¡å·¥ä½œé‡ï¼š**2 å‘¨**ï¼ˆ10 ä¸ªå·¥ä½œæ—¥ï¼‰
